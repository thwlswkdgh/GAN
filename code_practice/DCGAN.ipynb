{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1iEJQNH5lb5se3O0Wnhps70i41JqB7Z3c",
      "authorship_tag": "ABX9TyOFdcGeJd+RfTrd4Sif/BpV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thwlswkdgh/GAN/blob/main/code_practice/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경설정 \n",
        "- 데이터 폴더 생성\n",
        "- lsun git 저장소 clone\n",
        "- gan 소스 코드 clone\n"
      ],
      "metadata": {
        "id": "OdqmI81hw0HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /data\n",
        "!git clone https://github.com/fyu/lsun.git\n",
        "#!git clone https://github.com/PacktPublishing/Generative-Adversarial-Networks-Cookbook.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGeb0qKVwbq0",
        "outputId": "18f06328-0788-41cc-f7aa-b82d9e213b2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lsun' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download the data into our data folder"
      ],
      "metadata": {
        "id": "5XDeJlIswcaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4oEMz0QwB84",
        "outputId": "4ffd254d-e620-4824-a299-fadf1f68c381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading church_outdoor train set\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2335M  100 2335M    0     0  5433k      0  0:07:20  0:07:20 --:--:-- 4330k\n",
            "Downloading church_outdoor val set\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5817k  100 5817k    0     0  3052k      0  0:00:01  0:00:01 --:--:-- 3050k\n"
          ]
        }
      ],
      "source": [
        "!python lsun/download.py -o /data  -c church_outdoor "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /data/church_outdoor_train_lmdb.zip -d /data/\n",
        "!unzip /data/church_outdoor_val_lmdb.zip  -d  /data/\n",
        "!mkdir /data/church_outdoor_train_lmdb/expanded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYfFP0lC2WqY",
        "outputId": "48fdb1e6-33c5-4d40-c33a-f2142019d562"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /data/church_outdoor_train_lmdb.zip\n",
            "   creating: /data/church_outdoor_train_lmdb/\n",
            "  inflating: /data/church_outdoor_train_lmdb/lock.mdb  \n",
            "  inflating: /data/church_outdoor_train_lmdb/data.mdb  \n",
            "Archive:  /data/church_outdoor_val_lmdb.zip\n",
            "   creating: /data/church_outdoor_val_lmdb/\n",
            "  inflating: /data/church_outdoor_val_lmdb/lock.mdb  \n",
            "  inflating: /data/church_outdoor_val_lmdb/data.mdb  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Expand the data into our data folder"
      ],
      "metadata": {
        "id": "J67IlhVd0R1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python lsun/data.py export /data/church_outdoor_train_lmdb --out_dir /data/church_outdoor_train_lmdb/expanded --flat "
      ],
      "metadata": {
        "id": "Bkok1sv2zwfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Save to NPY File\n"
      ],
      "metadata": {
        "id": "9S4YmdDC1DE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/Generative-Adversarial-Networks-Cookbook/Chapter4/src/save_to_npy.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWehzVV81Eob",
        "outputId": "223a0292-ff3f-444f-a13b-ed48b0351cfc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ImageArr Gray:  (126227, 64, 64)\n",
            "tcmalloc: large alloc 1551081472 bytes == 0x564ed0dcc000 @  0x7f53c3cf61e7 0x7f53bf9c246e 0x7f53bfa12c7b 0x7f53bfa15e83 0x7f53bfa1607b 0x7f53bfab7761 0x564e400e0544 0x564e400e0240 0x564e40154627 0x564e4014e9ee 0x564e400e1bda 0x564e4014f915 0x564e40020d14 0x7f53bf9fffb3 0x564e400e0437 0x564e400e0240 0x564e40153973 0x564e4014e9ee 0x564e400e1bda 0x564e40153d00 0x564e4014e9ee 0x564e4014e6f3 0x564e402184c2 0x564e4021883d 0x564e402186e6 0x564e401f0163 0x564e401efe0c 0x7f53c2ae0bf7 0x564e401efcea\n",
            "Shape of ImageArr Color:  (126227, 64, 64, 3)\n",
            "tcmalloc: large alloc 1551081472 bytes == 0x564ed0dcc000 @  0x7f53c3cf61e7 0x7f53bf9c246e 0x7f53bfa12c7b 0x7f53bfa15e83 0x7f53bfa1607b 0x7f53bfab7761 0x564e400e0544 0x564e400e0240 0x564e40154627 0x564e4014e9ee 0x564e400e1bda 0x564e40153d00 0x564e4014e9ee 0x564e40020eb0 0x7f53bf9fffb3 0x564e400e0437 0x564e400e0240 0x564e40153973 0x564e4014e9ee 0x564e400e1bda 0x564e40153d00 0x564e4014e9ee 0x564e4014e6f3 0x564e402184c2 0x564e4021883d 0x564e402186e6 0x564e401f0163 0x564e401efe0c 0x7f53c2ae0bf7 0x564e401efcea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCGAN\n",
        "- run.py 실행"
      ],
      "metadata": {
        "id": "SXJ-S9CM7Cy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Reshape, Input, BatchNormalization\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Convolution2D, MaxPooling2D,Deconvolution2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Nadam,Adamax\n",
        "from keras import initializers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "class Generator(object):\n",
        "    def __init__(self, width = 28, height= 28, channels = 1, latent_size=100, model_type = 'simple'):\n",
        "        \n",
        "        self.W = width\n",
        "        self.H = height\n",
        "        self.C = channels\n",
        "        self.LATENT_SPACE_SIZE = latent_size\n",
        "        self.latent_space = np.random.normal(0,1,(self.LATENT_SPACE_SIZE,))\n",
        "\n",
        "        if model_type=='simple':\n",
        "            self.Generator = self.model()\n",
        "            self.OPTIMIZER = Adam(learning_rate=0.0002, decay=8e-9)\n",
        "            self.Generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
        "        elif model_type=='DCGAN':\n",
        "            self.Generator = self.dc_model()\n",
        "            self.OPTIMIZER = Adam(learning_rate=1e-4, beta_1=0.2)\n",
        "            self.Generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER,metrics=['accuracy'])\n",
        "        self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def dc_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256*8*8,activation=LeakyReLU(0.2), input_dim=self.LATENT_SPACE_SIZE))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Reshape((8, 8, 256)))\n",
        "        model.add(UpSampling2D())\n",
        "\n",
        "        model.add(Convolution2D(128, kernel_size= 5, padding='same',activation=LeakyReLU(0.2)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(UpSampling2D())\n",
        "\n",
        "        model.add(Convolution2D(64, kernel_size= 5, padding='same',activation=LeakyReLU(0.2)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(UpSampling2D())\n",
        "\n",
        "        model.add(Convolution2D(self.C, kernel_size= 5, padding='same', activation='tanh'))\n",
        "        \n",
        "        return model\n",
        "\n",
        "\n",
        "    def model(self, block_starting_size=128,num_blocks=4):\n",
        "        model = Sequential()\n",
        "        \n",
        "        block_size = block_starting_size \n",
        "        model.add(Dense(block_size, input_shape=(self.LATENT_SPACE_SIZE,)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        for i in range(num_blocks-1):\n",
        "            block_size = block_size * 2\n",
        "            model.add(Dense(block_size))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "            model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        model.add(Dense(self.W * self.H * self.C, activation='tanh'))\n",
        "        model.add(Reshape((self.W, self.H, self.C)))\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.Generator.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.Generator, to_file='/data/Generator_Model.png')"
      ],
      "metadata": {
        "id": "rbtLBO5dApjx"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Lambda, concatenate\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD,Nadam, Adamax\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "class Discriminator(object):\n",
        "    def __init__(self, width = 28, height= 28, channels = 1, latent_size=100,model_type = 'simple'):\n",
        "        self.W = width\n",
        "        self.H = height\n",
        "        self.C = channels\n",
        "        self.CAPACITY = width*height*channels\n",
        "        self.SHAPE = (width,height,channels)\n",
        "        \n",
        "        if model_type=='simple':\n",
        "            self.Discriminator = self.model()\n",
        "            self.OPTIMIZER = Adam(learning_rate=0.0002, decay=8e-9)\n",
        "            self.Discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
        "        elif model_type=='DCGAN':\n",
        "            self.Discriminator = self.dc_model()\n",
        "            self.OPTIMIZER = Adam(learning_rate=1e-4, beta_1=0.2)\n",
        "            self.Discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
        "\n",
        "        self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def dc_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Convolution2D(64, kernel_size= 5, strides= 2, input_shape=(self.W,self.H,self.C), padding='same',activation=LeakyReLU(alpha=0.2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Convolution2D(128, kernel_size= 5, strides= 2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        return model\n",
        "\n",
        "    def model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=self.SHAPE))\n",
        "        model.add(Dense(self.CAPACITY, input_shape=self.SHAPE))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(int(self.CAPACITY/2)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.Discriminator.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.Discriminator, to_file='/data/Discriminator_Model.png')\n",
        "\n"
      ],
      "metadata": {
        "id": "gUmulDzLC8ma"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "class GAN(object):\n",
        "    def __init__(self,discriminator,generator):\n",
        "        self.OPTIMIZER = SGD(learning_rate=2e-4,nesterov=True)\n",
        "        self.Generator = generator\n",
        "\n",
        "        self.Discriminator = discriminator\n",
        "        self.Discriminator.trainable = False\n",
        "        \n",
        "        self.gan_model = self.model()\n",
        "        self.gan_model.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
        "        self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def model(self):\n",
        "        model = Sequential()\n",
        "        model.add(self.Generator)\n",
        "        model.add(self.Discriminator)\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.gan_model.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.gan_model, to_file='/data/GAN_Model.png')\n",
        "\n"
      ],
      "metadata": {
        "id": "YPvaCjxkDpzn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "from keras.datasets import mnist\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, width = 28, height= 28, channels = 1, latent_size=100, epochs =50000, batch=32, checkpoint=50,model_type=-1,data_path = ''):\n",
        "        self.W = width\n",
        "        self.H = height\n",
        "        self.C = channels\n",
        "        self.EPOCHS = epochs\n",
        "        self.BATCH = batch\n",
        "        self.CHECKPOINT = checkpoint\n",
        "        self.model_type=model_type\n",
        "\n",
        "        self.LATENT_SPACE_SIZE = latent_size\n",
        "\n",
        "        self.generator = Generator(height=self.H, width=self.W, channels=self.C, latent_size=self.LATENT_SPACE_SIZE,model_type = 'DCGAN')\n",
        "        self.discriminator = Discriminator(height=self.H, width=self.W, channels=self.C,model_type = 'DCGAN')\n",
        "        self.gan = GAN(generator=self.generator.Generator, discriminator=self.discriminator.Discriminator)\n",
        "\n",
        "        #self.load_MNIST()\n",
        "        self.load_npy(data_path)\n",
        "\n",
        "    def load_npy(self,npy_path):\n",
        "        self.X_train = np.load(npy_path)\n",
        "        self.X_train = self.X_train[:int(0.25*float(len(self.X_train)))]\n",
        "        self.X_train = (self.X_train.astype(np.float32) - 127.5)/127.5\n",
        "        self.X_train = np.expand_dims(self.X_train, axis=3)\n",
        "        return\n",
        "\n",
        "    def load_MNIST(self,model_type=3):\n",
        "        allowed_types = [-1,0,1,2,3,4,5,6,7,8,9]\n",
        "        if self.model_type not in allowed_types:\n",
        "            print('ERROR: Only Integer Values from -1 to 9 are allowed')\n",
        "\n",
        "        (self.X_train, self.Y_train), (_, _) = mnist.load_data()\n",
        "        if self.model_type!=-1:\n",
        "            self.X_train = self.X_train[np.where(self.Y_train==int(self.model_type))[0]]\n",
        "        \n",
        "        # Rescale -1 to 1\n",
        "        # Find Normalize Function from CV Class  \n",
        "        self.X_train = ( np.float32(self.X_train) - 127.5) / 127.5\n",
        "        self.X_train = np.expand_dims(self.X_train, axis=3)\n",
        "        return\n",
        "\n",
        "    def train(self):\n",
        "        for e in range(self.EPOCHS):\n",
        "            b = 0\n",
        "            X_train_temp = deepcopy(self.X_train)\n",
        "            while len(X_train_temp)>self.BATCH:\n",
        "                # Keep track of Batches\n",
        "                b=b+1\n",
        "\n",
        "                # Train Discriminator\n",
        "                # Make the training batch for this model be half real, half noise\n",
        "                # Grab Real Images for this training batch\n",
        "                if self.flipCoin():\n",
        "                    count_real_images = int(self.BATCH)\n",
        "                    starting_index = randint(0, (len(X_train_temp)-count_real_images))\n",
        "                    real_images_raw = X_train_temp[ starting_index : (starting_index + count_real_images) ]\n",
        "                    #self.plot_check_batch(b,real_images_raw)\n",
        "                    # Delete the images used until we have none left\n",
        "                    X_train_temp = np.delete(X_train_temp,range(starting_index,(starting_index + count_real_images)),0)\n",
        "                    x_batch = real_images_raw.reshape( count_real_images, self.W, self.H, self.C )\n",
        "                    y_batch = np.ones([count_real_images,1])\n",
        "                else:\n",
        "                    # Grab Generated Images for this training batch\n",
        "                    latent_space_samples = self.sample_latent_space(self.BATCH)\n",
        "                    x_batch = self.generator.Generator.predict(latent_space_samples)\n",
        "                    y_batch = np.zeros([self.BATCH,1])\n",
        "\n",
        "                # Now, train the discriminator with this batch\n",
        "                discriminator_loss = self.discriminator.Discriminator.train_on_batch(x_batch,y_batch)[0]\n",
        "            \n",
        "                # In practice, flipping the label when training the generator improves convergence\n",
        "                if self.flipCoin(chance=0.9):\n",
        "                    y_generated_labels = np.ones([self.BATCH,1])\n",
        "                else:\n",
        "                    y_generated_labels = np.zeros([self.BATCH,1])\n",
        "                x_latent_space_samples = self.sample_latent_space(self.BATCH)\n",
        "                generator_loss = self.gan.gan_model.train_on_batch(x_latent_space_samples,y_generated_labels)\n",
        "    \n",
        "                print ('Batch: '+str(int(b))+', [Discriminator :: Loss: '+str(discriminator_loss)+'], [ Generator :: Loss: '+str(generator_loss)+']')\n",
        "                if b % self.CHECKPOINT == 0 :\n",
        "                    label = str(e)+'_'+str(b)\n",
        "                    self.plot_checkpoint(label)\n",
        "\n",
        "            print ('Epoch: '+str(int(e))+', [Discriminator :: Loss: '+str(discriminator_loss)+'], [ Generator :: Loss: '+str(generator_loss)+']')\n",
        "                        \n",
        "            if e % self.CHECKPOINT == 0 :\n",
        "                self.plot_checkpoint(e)\n",
        "        return\n",
        "\n",
        "    def flipCoin(self,chance=0.5):\n",
        "        return np.random.binomial(1, chance)\n",
        "\n",
        "    def sample_latent_space(self, instances):\n",
        "        return np.random.normal(0, 1, (instances,self.LATENT_SPACE_SIZE))\n",
        "\n",
        "    def plot_checkpoint(self,e):\n",
        "        filename = \"/data/sample_\"+str(e)+\".png\"\n",
        "\n",
        "        noise = self.sample_latent_space(16)\n",
        "        images = self.generator.Generator.predict(noise)\n",
        "        \n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            if self.C==1:\n",
        "                image = images[i, :, :]\n",
        "                image = np.reshape(image, [self.H,self.W])\n",
        "                image = (255*(image - np.min(image))/np.ptp(image)).astype(int)\n",
        "                plt.imshow(image,cmap='gray')\n",
        "            elif self.C==3:\n",
        "                image = images[i, :, :, :]\n",
        "                image = np.reshape(image, [self.H,self.W,self.C])\n",
        "                image = (255*(image - np.min(image))/np.ptp(image)).astype(int)\n",
        "                plt.imshow(image)\n",
        "            \n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filename)\n",
        "        plt.close('all')\n",
        "        return\n",
        "\n",
        "    def plot_check_batch(self,b,images):\n",
        "        filename = \"/data/batch_check_\"+str(b)+\".png\"\n",
        "        subplot_size = int(np.sqrt(images.shape[0]))\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(subplot_size, subplot_size, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.H,self.W,self.C])\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filename)\n",
        "        plt.close('all')\n",
        "        return\n"
      ],
      "metadata": {
        "id": "HvyBTVN5Dw3v"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# Command Line Argument Method\n",
        "HEIGHT  = 64\n",
        "WIDTH   = 64\n",
        "CHANNEL = 3\n",
        "LATENT_SPACE_SIZE = 100\n",
        "EPOCHS = 100\n",
        "BATCH = 128\n",
        "CHECKPOINT = 10\n",
        "PATH = \"/data/church_outdoor_train_lmdb_color.npy\"\n",
        "\n",
        "trainer = Trainer(height=HEIGHT,\\\n",
        "                 width=WIDTH,\\\n",
        "                 channels=CHANNEL,\\\n",
        "                 latent_size=LATENT_SPACE_SIZE,\\\n",
        "                 epochs =EPOCHS,\\\n",
        "                 batch=BATCH,\\\n",
        "                 checkpoint=CHECKPOINT,\\\n",
        "                 model_type='DCGAN',\\\n",
        "                 data_path=PATH)\n",
        "                 \n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgX8t3lsD4pI",
        "outputId": "abdc8705-e17c-4e93-c6f0-5447075ed2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_24 (Dense)            (None, 16384)             1654784   \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 16384)            65536     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " reshape_9 (Reshape)         (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " up_sampling2d_16 (UpSamplin  (None, 16, 16, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 16, 16, 128)       819328    \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 16, 16, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " up_sampling2d_17 (UpSamplin  (None, 32, 32, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 32, 32, 64)        204864    \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 32, 32, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " up_sampling2d_18 (UpSamplin  (None, 64, 64, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 64, 64, 3)         4803      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,750,083\n",
            "Trainable params: 2,716,931\n",
            "Non-trainable params: 33,152\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 32, 32, 64)        4864      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 32, 32, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 128)       204928    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 16, 16, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 243,329\n",
            "Trainable params: 242,945\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_16 (Sequential)  (None, 64, 64, 3)         2750083   \n",
            "                                                                 \n",
            " sequential_17 (Sequential)  (None, 1)                 243329    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,993,412\n",
            "Trainable params: 2,716,931\n",
            "Non-trainable params: 276,481\n",
            "_________________________________________________________________\n",
            "Batch: 1, [Discriminator :: Loss: 0.8029841184616089], [ Generator :: Loss: 0.7148033976554871]\n",
            "Batch: 2, [Discriminator :: Loss: 0.7940331697463989], [ Generator :: Loss: 0.7213762402534485]\n",
            "Batch: 3, [Discriminator :: Loss: 0.6230862140655518], [ Generator :: Loss: 0.7392706871032715]\n",
            "Batch: 4, [Discriminator :: Loss: 0.672724723815918], [ Generator :: Loss: 0.7489116787910461]\n",
            "Batch: 5, [Discriminator :: Loss: 0.5098317861557007], [ Generator :: Loss: 0.7703480124473572]\n",
            "Batch: 6, [Discriminator :: Loss: 0.41917353868484497], [ Generator :: Loss: 0.7636609077453613]\n",
            "Batch: 7, [Discriminator :: Loss: 0.3245120942592621], [ Generator :: Loss: 0.7570716142654419]\n",
            "Batch: 8, [Discriminator :: Loss: 0.30684593319892883], [ Generator :: Loss: 0.7720614671707153]\n",
            "Batch: 9, [Discriminator :: Loss: 0.4279841184616089], [ Generator :: Loss: 0.6269873380661011]\n",
            "Batch: 10, [Discriminator :: Loss: 0.4072487950325012], [ Generator :: Loss: 0.7988024950027466]\n",
            "Batch: 11, [Discriminator :: Loss: 0.17870965600013733], [ Generator :: Loss: 0.780638575553894]\n",
            "Batch: 12, [Discriminator :: Loss: 0.34714919328689575], [ Generator :: Loss: 0.5971584320068359]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GL-1GMrwD_CQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}