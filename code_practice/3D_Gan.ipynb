{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thwlswkdgh/GAN/blob/main/code_practice/3D_Gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGCF4R9w_gsw",
        "outputId": "a7250b8c-21bc-49cf-ca17-b4fa8c4b48ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/100\n",
            "235/235 [==============================] - 11s 42ms/step - loss: 0.6951 - val_loss: 0.6946\n",
            "Epoch 2/100\n",
            "235/235 [==============================] - 9s 39ms/step - loss: 0.6940 - val_loss: 0.6935\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.6930 - val_loss: 0.6925\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6920 - val_loss: 0.6915\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6910 - val_loss: 0.6905\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6900 - val_loss: 0.6895\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6890 - val_loss: 0.6885\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6881 - val_loss: 0.6875\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6871 - val_loss: 0.6865\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6861 - val_loss: 0.6855\n",
            "Epoch 11/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6851 - val_loss: 0.6845\n",
            "Epoch 12/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6841 - val_loss: 0.6835\n",
            "Epoch 13/100\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.6831 - val_loss: 0.6825\n",
            "Epoch 14/100\n",
            "235/235 [==============================] - 10s 43ms/step - loss: 0.6821 - val_loss: 0.6814\n",
            "Epoch 15/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.6812 - val_loss: 0.6804\n",
            "Epoch 16/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6802 - val_loss: 0.6794\n",
            "Epoch 17/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6791 - val_loss: 0.6784\n",
            "Epoch 18/100\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.6781 - val_loss: 0.6773\n",
            "Epoch 19/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6771 - val_loss: 0.6763\n",
            "Epoch 20/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6761 - val_loss: 0.6752\n",
            "Epoch 21/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6750 - val_loss: 0.6742\n",
            "Epoch 22/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6740 - val_loss: 0.6731\n",
            "Epoch 23/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6729 - val_loss: 0.6720\n",
            "Epoch 24/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6718 - val_loss: 0.6708\n",
            "Epoch 25/100\n",
            "235/235 [==============================] - 6s 27ms/step - loss: 0.6707 - val_loss: 0.6697\n",
            "Epoch 26/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6696 - val_loss: 0.6686\n",
            "Epoch 27/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6684 - val_loss: 0.6674\n",
            "Epoch 28/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6673 - val_loss: 0.6662\n",
            "Epoch 29/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6661 - val_loss: 0.6650\n",
            "Epoch 30/100\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.6649 - val_loss: 0.6638\n",
            "Epoch 31/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6637 - val_loss: 0.6625\n",
            "Epoch 32/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6624 - val_loss: 0.6612\n",
            "Epoch 33/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6611 - val_loss: 0.6599\n",
            "Epoch 34/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6598 - val_loss: 0.6586\n",
            "Epoch 35/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6585 - val_loss: 0.6572\n",
            "Epoch 36/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.6572 - val_loss: 0.6558\n",
            "Epoch 37/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6558 - val_loss: 0.6544\n",
            "Epoch 38/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6544 - val_loss: 0.6530\n",
            "Epoch 39/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6530 - val_loss: 0.6515\n",
            "Epoch 40/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6515 - val_loss: 0.6500\n",
            "Epoch 41/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6500 - val_loss: 0.6484\n",
            "Epoch 42/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6485 - val_loss: 0.6469\n",
            "Epoch 43/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6469 - val_loss: 0.6452\n",
            "Epoch 44/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6453 - val_loss: 0.6436\n",
            "Epoch 45/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6437 - val_loss: 0.6419\n",
            "Epoch 46/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6420 - val_loss: 0.6402\n",
            "Epoch 47/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6403 - val_loss: 0.6384\n",
            "Epoch 48/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6385 - val_loss: 0.6367\n",
            "Epoch 49/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6367 - val_loss: 0.6348\n",
            "Epoch 50/100\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.6349 - val_loss: 0.6329\n",
            "Epoch 51/100\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.6331 - val_loss: 0.6310\n",
            "Epoch 52/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6312 - val_loss: 0.6291\n",
            "Epoch 53/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6292 - val_loss: 0.6271\n",
            "Epoch 54/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6272 - val_loss: 0.6250\n",
            "Epoch 55/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6252 - val_loss: 0.6230\n",
            "Epoch 56/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6231 - val_loss: 0.6208\n",
            "Epoch 57/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6210 - val_loss: 0.6187\n",
            "Epoch 58/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.6189 - val_loss: 0.6165\n",
            "Epoch 59/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6167 - val_loss: 0.6142\n",
            "Epoch 60/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6144 - val_loss: 0.6119\n",
            "Epoch 61/100\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.6121 - val_loss: 0.6096\n",
            "Epoch 62/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6098 - val_loss: 0.6072\n",
            "Epoch 63/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6074 - val_loss: 0.6047\n",
            "Epoch 64/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.6050 - val_loss: 0.6023\n",
            "Epoch 65/100\n",
            "235/235 [==============================] - 8s 32ms/step - loss: 0.6026 - val_loss: 0.5997\n",
            "Epoch 66/100\n",
            "235/235 [==============================] - 6s 27ms/step - loss: 0.6001 - val_loss: 0.5972\n",
            "Epoch 67/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5975 - val_loss: 0.5946\n",
            "Epoch 68/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5949 - val_loss: 0.5919\n",
            "Epoch 69/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5923 - val_loss: 0.5892\n",
            "Epoch 70/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5896 - val_loss: 0.5865\n",
            "Epoch 71/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5869 - val_loss: 0.5837\n",
            "Epoch 72/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5841 - val_loss: 0.5809\n",
            "Epoch 73/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5813 - val_loss: 0.5780\n",
            "Epoch 74/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5785 - val_loss: 0.5751\n",
            "Epoch 75/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5756 - val_loss: 0.5722\n",
            "Epoch 76/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5727 - val_loss: 0.5692\n",
            "Epoch 77/100\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.5697 - val_loss: 0.5662\n",
            "Epoch 78/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5667 - val_loss: 0.5632\n",
            "Epoch 79/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5637 - val_loss: 0.5601\n",
            "Epoch 80/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5607 - val_loss: 0.5570\n",
            "Epoch 81/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5576 - val_loss: 0.5539\n",
            "Epoch 82/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5545 - val_loss: 0.5507\n",
            "Epoch 83/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5513 - val_loss: 0.5475\n",
            "Epoch 84/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5481 - val_loss: 0.5443\n",
            "Epoch 85/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5449 - val_loss: 0.5410\n",
            "Epoch 86/100\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 0.5417 - val_loss: 0.5378\n",
            "Epoch 87/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5385 - val_loss: 0.5345\n",
            "Epoch 88/100\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 0.5352 - val_loss: 0.5312\n",
            "Epoch 89/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5319 - val_loss: 0.5278\n",
            "Epoch 90/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5286 - val_loss: 0.5245\n",
            "Epoch 91/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5253 - val_loss: 0.5211\n",
            "Epoch 92/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5220 - val_loss: 0.5178\n",
            "Epoch 93/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5186 - val_loss: 0.5144\n",
            "Epoch 94/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5153 - val_loss: 0.5110\n",
            "Epoch 95/100\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.5119 - val_loss: 0.5076\n",
            "Epoch 96/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.5086 - val_loss: 0.5043\n",
            "Epoch 97/100\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.5052 - val_loss: 0.5009\n",
            "Epoch 98/100\n",
            "235/235 [==============================] - 7s 31ms/step - loss: 0.5019 - val_loss: 0.4975\n",
            "Epoch 99/100\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.4985 - val_loss: 0.4941\n",
            "Epoch 100/100\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.4952 - val_loss: 0.4907\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "#encoder\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# Download the data and format for learning\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# How much encoding do we want for our setup?\n",
        "encoding_dimension = 256  \n",
        "\n",
        "# Keras has an input shape of 784\n",
        "input_layer = Input(shape=(784,))\n",
        "encoded_layer = Dense(encoding_dimension, activation='relu')(input_layer)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded_layer)\n",
        "\n",
        "# Build the Model\n",
        "ac = Model(input_layer, decoded)\n",
        "\n",
        "# Create an encoder model that we will save later\n",
        "encoder = Model(input_layer, encoded_layer)\n",
        "\n",
        "# Train the autoencoder model, ac\n",
        "ac.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "ac.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "# Save the Predicted Data x_train\n",
        "x_train_encoded = encoder.predict(x_train)\n",
        "np.save('x_train_encoded.npy',x_train_encoded)\n",
        "# Save the Predicted Data x_test\n",
        "x_test_encoded = encoder.predict(x_test)\n",
        "np.save('x_test_encoded.npy',x_test_encoded)\n",
        "# Save the Encoder model\n",
        "encoder.save('encoder_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "gMUgDof3t2Uk",
        "outputId": "32fc5750-a98e-461e-a29c-c8fd9ce9e107"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-52b4f799-be5e-401a-9cc5-6d95da3523bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-52b4f799-be5e-401a-9cc5-6d95da3523bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKA2O7C_uEo1",
        "outputId": "c7cacb69-c566-46f4-90bf-d16b48d21ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 3d-mnist.zip to /content\n",
            " 98% 149M/153M [00:02<00:00, 67.6MB/s]\n",
            "100% 153M/153M [00:02<00:00, 65.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d daavoo/3d-mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jOo8wWuuZb3",
        "outputId": "bc9f1899-16a8-4f7a-a6ab-0e303a25e3b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  3d-mnist.zip\n",
            "  inflating: 3d-mnist/full_dataset_vectors.h5  \n",
            "  inflating: 3d-mnist/plot3D.py      \n",
            "  inflating: 3d-mnist/test_point_clouds.h5  \n",
            "  inflating: 3d-mnist/train_point_clouds.h5  \n",
            "  inflating: 3d-mnist/voxelgrid.py   \n"
          ]
        }
      ],
      "source": [
        "!unzip 3d-mnist -d 3d-mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng4VnTbJ_kbj"
      },
      "outputs": [],
      "source": [
        "#discriminator\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers.convolutional import Conv3D, Deconv3D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "class Discriminator(object):\n",
        "    def __init__(self, side=16):\n",
        "        self.INPUT_SHAPE = (side,side,side,3)\n",
        "        self.OPTIMIZER = Adam(lr=0.000001, beta_1=0.5)\n",
        "\n",
        "\n",
        "        self.Discriminator = self.model()\n",
        "        self.Discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
        "        # self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def block(self,first_layer,filter_size=512,kernel_size=(3,3,3)):\n",
        "\n",
        "        x = Conv3D(filters=filter_size, kernel_size=kernel_size, kernel_initializer='glorot_normal',\n",
        "                    bias_initializer='zeros', padding='same')(first_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def model(self):\n",
        "        input_layer = Input(shape=self.INPUT_SHAPE)\n",
        "        x = self.block(input_layer,filter_size=8)\n",
        "        x = self.block(x,filter_size=16,)\n",
        "        x = self.block(x,filter_size=32)\n",
        "        x = self.block(x,filter_size=64)\n",
        "\n",
        "\n",
        "        x = Conv3D(filters=1, kernel_size=(3,3,3),\n",
        "                    strides=(1,1,1), kernel_initializer='glorot_normal',\n",
        "                    bias_initializer='zeros', padding='valid')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Flatten()(x)\n",
        "        output_layer = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.Discriminator.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.Discriminator.model, to_file='Discriminator_Model.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFuTShoI_pze"
      },
      "outputs": [],
      "source": [
        "# generator\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Conv3D, Deconv3D\n",
        "from keras.layers import Input, BatchNormalization, Dense, Reshape\n",
        "from keras.layers.core import Activation\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "# from keras.utils import plot_model\n",
        "\n",
        "class Generator(object):\n",
        "    def __init__(self, latent_size=100):\n",
        "\n",
        "        self.INPUT_SHAPE = (1, 1, 1, latent_size)\n",
        "        # self.OPTIMIZER = Adam(lr=0.0001,beta_1=0.5)\n",
        "        self.OPTIMIZER = SGD(lr=0.001, nesterov=True)\n",
        "\n",
        "\n",
        "        self.Generator = self.model()\n",
        "        self.Generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
        "        # self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def block(self,first_layer,filter_size=512,stride_size=(2,2,2),kernel_size=(4,4,4),padding='same'):\n",
        "\n",
        "        x = Deconv3D(filters=filter_size, kernel_size=kernel_size,\n",
        "                    strides=stride_size, kernel_initializer='glorot_normal',\n",
        "                    bias_initializer='zeros', padding=padding)(first_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(activation='relu')(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def model(self):\n",
        "        input_layer = Input(shape=self.INPUT_SHAPE)\n",
        "\n",
        "        x = self.block(input_layer,filter_size=256,stride_size=(1,1,1),kernel_size=(4,4,4),padding='valid')\n",
        "        x = self.block(x,filter_size=128,stride_size=(2,2,2),kernel_size=(4,4,4))\n",
        "\n",
        "        x = Deconv3D(filters=3, kernel_size=(4,4,4),\n",
        "                    strides=(2,2,2), kernel_initializer='glorot_normal',\n",
        "                    bias_initializer='zeros', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        output_layer = Activation(activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output_layer)\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.Generator.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.Generator.model, to_file='Generator_Model.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziGnQNbQ_tLO"
      },
      "outputs": [],
      "source": [
        "#gan\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils import plot_model\n",
        "\n",
        "class GAN(object):\n",
        "    def __init__(self,discriminator,generator):\n",
        "        self.OPTIMIZER = Adam(lr=0.008, beta_1=0.5)\n",
        "        \n",
        "        self.Generator = generator\n",
        "        self.Discriminator = discriminator\n",
        "        self.Discriminator.trainable = True\n",
        "        \n",
        "        self.gan_model = self.model()\n",
        "        self.gan_model.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
        "        # self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def model(self):\n",
        "        model = Sequential()\n",
        "        model.add(self.Generator)\n",
        "        model.add(self.Discriminator)\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.gan_model.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.gan_model.model, to_file='GAN_Model.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohR_lUj4_2J4"
      },
      "outputs": [],
      "source": [
        "#train\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "from keras.datasets import mnist\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py    \n",
        "# This import registers the 3D projection, but is otherwise unused.\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
        "# from voxelgrid import VoxelGrid\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, side=16, latent_size=32, epochs =100, batch=32, checkpoint=50, data_dir = ''):\n",
        "        self.SIDE=side\n",
        "        self.EPOCHS = epochs\n",
        "        self.BATCH = batch\n",
        "        self.CHECKPOINT = checkpoint\n",
        "\n",
        "        self.load_3D_MNIST(data_dir)\n",
        "        self.load_2D_encoded_MNIST()\n",
        "        self.LATENT_SPACE_SIZE = latent_size\n",
        "        self.LABELS = [1]\n",
        "\n",
        "        self.generator = Generator(latent_size=self.LATENT_SPACE_SIZE)\n",
        "        self.discriminator = Discriminator(side=self.SIDE)\n",
        "        self.gan = GAN(generator=self.generator.Generator, discriminator=self.discriminator.Discriminator)\n",
        "        \n",
        "    # Translate data to color\n",
        "    def array_to_color(self,array, cmap=\"Oranges\"):\n",
        "        s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
        "        return s_m.to_rgba(array)[:,:-1]\n",
        "\n",
        "    def translate(self,x):\n",
        "        xx = np.ndarray((x.shape[0], 4096, 3))\n",
        "        for i in range(x.shape[0]):\n",
        "            xx[i] = self.array_to_color(x[i])\n",
        "        del x\n",
        "        return xx\n",
        "    def load_3D_MNIST(self,input_dir):\n",
        "        raw = h5py.File(input_dir, 'r')\n",
        "\n",
        "\n",
        "        self.X_train_3D = np.array(raw['X_train'])\n",
        "        self.X_train_3D = ( np.float32(self.X_train_3D) - 127.5) / 127.5\n",
        "        self.X_train_3D = self.translate(self.X_train_3D).reshape(-1, 16, 16, 16, 3)\n",
        "        self.X_test_3D = np.array(raw['X_test'])\n",
        "        self.X_test_3D = ( np.float32(self.X_test_3D) - 127.5) / 127.5\n",
        "        self.X_test_3D = self.translate(self.X_test_3D).reshape(-1, 16, 16, 16, 3)\n",
        "        self.Y_train_3D = np.array(raw['y_train'])\n",
        "        self.Y_test_3D = np.array(raw['y_test'])\n",
        "\n",
        "        return \n",
        "\n",
        "    def load_2D_encoded_MNIST(self):\n",
        "        (_, self.Y_train_2D), (_, self.Y_test_2D) = mnist.load_data()\n",
        "        self.X_train_2D_encoded = np.load('x_train_encoded.npy')\n",
        "        self.X_test_2D_encoded = np.load('x_test_encoded.npy')\n",
        "        return\n",
        "\n",
        "    def train(self):\n",
        "        \n",
        "        count_generated_images = int(self.BATCH/2)\n",
        "        count_real_images = int(self.BATCH/2)\n",
        "        for e in range(self.EPOCHS):\n",
        "            for label in self.LABELS:\n",
        "\n",
        "                # Grab the Real 3D Samples\n",
        "                all_3D_samples = self.X_train_3D[np.where(self.Y_train_3D==label)]\n",
        "                starting_index = randint(0, (len(all_3D_samples)-count_real_images))\n",
        "                real_3D_samples = all_3D_samples[ starting_index : int((starting_index + count_real_images)) ]\n",
        "                y_real_labels =  np.ones([count_generated_images,1])\n",
        "\n",
        "                # Grab Generated Images for this training batch\n",
        "                all_encoded_samples = self.X_train_2D_encoded[np.where(self.Y_train_2D==label)]\n",
        "                starting_index = randint(0, (len(all_encoded_samples)-count_generated_images))\n",
        "                batch_encoded_samples = all_encoded_samples[ starting_index : int((starting_index + count_generated_images)) ]\n",
        "                batch_encoded_samples = batch_encoded_samples.reshape( count_generated_images, 1, 1, 1,self.LATENT_SPACE_SIZE)\n",
        "\n",
        "                x_generated_3D_samples = self.generator.Generator.predict(batch_encoded_samples)\n",
        "                y_generated_labels = np.zeros([count_generated_images,1])\n",
        "\n",
        "                # Combine to train on the discriminator\n",
        "                x_batch = np.concatenate( [real_3D_samples, x_generated_3D_samples] )\n",
        "                y_batch = np.concatenate( [y_real_labels, y_generated_labels] )\n",
        "\n",
        "                # Now, train the discriminator with this batch\n",
        "                self.discriminator.Discriminator.trainable = False\n",
        "                discriminator_loss = self.discriminator.Discriminator.train_on_batch(x_batch,y_batch)[0]\n",
        "                self.discriminator.Discriminator.trainable = True\n",
        "\n",
        "                # Generate Noise\n",
        "                starting_index = randint(0, (len(all_encoded_samples)-self.BATCH))\n",
        "                x_batch_encoded_samples = all_encoded_samples[ starting_index : int((starting_index + self.BATCH)) ]\n",
        "                x_batch_encoded_samples = x_batch_encoded_samples.reshape( int(self.BATCH), 1, 1, 1,self.LATENT_SPACE_SIZE)\n",
        "                y_generated_labels = np.ones([self.BATCH,1])\n",
        "                generator_loss = self.gan.gan_model.train_on_batch(x_batch_encoded_samples,y_generated_labels)\n",
        "                print ('Epoch: '+str(int(e))+' Label: '+str(int(label))+', [Discriminator :: Loss: '+str(discriminator_loss)+'], [ Generator :: Loss: '+str(generator_loss)+']')\n",
        "                if e % self.CHECKPOINT == 0 and e != 0 :\n",
        "                    self.plot_checkpoint(e,label)\n",
        "            \n",
        "        return\n",
        "\n",
        "    def plot_checkpoint(self,e,label):\n",
        "        filename = \"/content/out/epoch_\"+str(e)+\"_label_\"+str(label)+\".png\"\n",
        "\n",
        "        all_encoded_samples = self.X_test_2D_encoded[np.where(self.Y_test_2D==label)]\n",
        "        index = randint(0, (len(all_encoded_samples)-1))\n",
        "        batch_encoded_samples = all_encoded_samples[ index ]\n",
        "        batch_encoded_samples = batch_encoded_samples.reshape( 1, 1, 1, 1,self.LATENT_SPACE_SIZE)\n",
        "\n",
        "        images = self.generator.Generator.predict(batch_encoded_samples)\n",
        "        xs = []\n",
        "        ys = []\n",
        "        zs = []\n",
        "        cs = []\n",
        "        for i in range(16):\n",
        "            for j in range(16):\n",
        "                for k in range(16):\n",
        "                    color = images[0][i][j][k]\n",
        "                    if np.mean(color)<0.75 and np.mean(color)>0.25:\n",
        "                        xs.append(i)\n",
        "                        ys.append(j)\n",
        "                        zs.append(k)\n",
        "                        cs.append(color)\n",
        "\n",
        "        fig = plt.figure()\n",
        "        ax = fig.gca(projection='3d')\n",
        "        ax.scatter(xs,ys,zs,alpha=0.1,c=cs)\n",
        "        plt.savefig(filename)\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqCz1a9RF-Is",
        "outputId": "5b2e9106-a405-4b1f-afad-fdc5aea33e6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1, 1, 1, 256)]    0         \n",
            "                                                                 \n",
            " conv3d_transpose_3 (Conv3DT  (None, 4, 4, 4, 256)     4194560   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 4, 4, 4, 256)     1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 4, 4, 4, 256)      0         \n",
            "                                                                 \n",
            " conv3d_transpose_4 (Conv3DT  (None, 8, 8, 8, 128)     2097280   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 8, 8, 8, 128)     512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8, 8, 8, 128)      0         \n",
            "                                                                 \n",
            " conv3d_transpose_5 (Conv3DT  (None, 16, 16, 16, 3)    24579     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16, 16, 16, 3)    12        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16, 16, 16, 3)     0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,317,967\n",
            "Trainable params: 6,317,193\n",
            "Non-trainable params: 774\n",
            "_________________________________________________________________\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 16, 16, 16, 3)]   0         \n",
            "                                                                 \n",
            " conv3d_5 (Conv3D)           (None, 16, 16, 16, 8)     656       \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16, 16, 16, 8)    32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 16, 8)     0         \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                 \n",
            " conv3d_6 (Conv3D)           (None, 16, 16, 16, 16)    3472      \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 16, 16, 16, 16)   64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 16, 16)    0         \n",
            "                                                                 \n",
            " conv3d_7 (Conv3D)           (None, 16, 16, 16, 32)    13856     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 16, 16, 16, 32)   128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 16, 32)    0         \n",
            "                                                                 \n",
            " conv3d_8 (Conv3D)           (None, 16, 16, 16, 64)    55360     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 16, 16, 16, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 16, 64)    0         \n",
            "                                                                 \n",
            " conv3d_9 (Conv3D)           (None, 14, 14, 14, 1)     1729      \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 14, 14, 14, 1)    4         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2744)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 2745      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 78,302\n",
            "Trainable params: 78,060\n",
            "Non-trainable params: 242\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model_4 (Functional)        (None, 16, 16, 16, 3)     6317967   \n",
            "                                                                 \n",
            " model_5 (Functional)        (None, 1)                 78302     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,396,269\n",
            "Trainable params: 6,395,253\n",
            "Non-trainable params: 1,016\n",
            "_________________________________________________________________\n",
            "Epoch: 0 Label: 1, [Discriminator :: Loss: 0.521442174911499], [ Generator :: Loss: 0.8125659227371216]\n",
            "Epoch: 1 Label: 1, [Discriminator :: Loss: 3.9412412643432617], [ Generator :: Loss: 0.00012149459507782012]\n",
            "Epoch: 2 Label: 1, [Discriminator :: Loss: 3.784447193145752], [ Generator :: Loss: 3.475544508546591e-05]\n",
            "Epoch: 3 Label: 1, [Discriminator :: Loss: 3.89017915725708], [ Generator :: Loss: 1.7591968571650796e-05]\n",
            "Epoch: 4 Label: 1, [Discriminator :: Loss: 4.008048057556152], [ Generator :: Loss: 1.039581184159033e-05]\n",
            "Epoch: 5 Label: 1, [Discriminator :: Loss: 4.078862190246582], [ Generator :: Loss: 8.315046216011979e-06]\n",
            "Epoch: 6 Label: 1, [Discriminator :: Loss: 4.133573532104492], [ Generator :: Loss: 7.715447281952947e-06]\n",
            "Epoch: 7 Label: 1, [Discriminator :: Loss: 4.1786346435546875], [ Generator :: Loss: 7.221226042020135e-06]\n",
            "Epoch: 8 Label: 1, [Discriminator :: Loss: 4.174464702606201], [ Generator :: Loss: 6.9207717388053425e-06]\n",
            "Epoch: 9 Label: 1, [Discriminator :: Loss: 4.180875301361084], [ Generator :: Loss: 7.00557802701951e-06]\n",
            "Epoch: 10 Label: 1, [Discriminator :: Loss: 4.192115783691406], [ Generator :: Loss: 6.237397428776603e-06]\n",
            "Epoch: 11 Label: 1, [Discriminator :: Loss: 4.232183456420898], [ Generator :: Loss: 6.6580469137988985e-06]\n",
            "Epoch: 12 Label: 1, [Discriminator :: Loss: 4.30471134185791], [ Generator :: Loss: 6.259992005652748e-06]\n",
            "Epoch: 13 Label: 1, [Discriminator :: Loss: 4.306727409362793], [ Generator :: Loss: 6.514911547128577e-06]\n",
            "Epoch: 14 Label: 1, [Discriminator :: Loss: 4.329082489013672], [ Generator :: Loss: 6.8726599238289054e-06]\n",
            "Epoch: 15 Label: 1, [Discriminator :: Loss: 4.317283630371094], [ Generator :: Loss: 6.309938271442661e-06]\n",
            "Epoch: 16 Label: 1, [Discriminator :: Loss: 4.465507984161377], [ Generator :: Loss: 6.314839993137866e-06]\n",
            "Epoch: 17 Label: 1, [Discriminator :: Loss: 4.375678539276123], [ Generator :: Loss: 6.0901847973582335e-06]\n",
            "Epoch: 18 Label: 1, [Discriminator :: Loss: 4.41885232925415], [ Generator :: Loss: 6.405232397810323e-06]\n",
            "Epoch: 19 Label: 1, [Discriminator :: Loss: 4.458594799041748], [ Generator :: Loss: 6.303541795205092e-06]\n",
            "Epoch: 20 Label: 1, [Discriminator :: Loss: 4.468433380126953], [ Generator :: Loss: 5.785388566437177e-06]\n",
            "Epoch: 21 Label: 1, [Discriminator :: Loss: 4.4197258949279785], [ Generator :: Loss: 6.674185897281859e-06]\n",
            "Epoch: 22 Label: 1, [Discriminator :: Loss: 4.5083208084106445], [ Generator :: Loss: 5.940300070506055e-06]\n",
            "Epoch: 23 Label: 1, [Discriminator :: Loss: 4.546202659606934], [ Generator :: Loss: 5.625887752103154e-06]\n",
            "Epoch: 24 Label: 1, [Discriminator :: Loss: 4.522322654724121], [ Generator :: Loss: 5.925040568399709e-06]\n",
            "Epoch: 25 Label: 1, [Discriminator :: Loss: 4.660604953765869], [ Generator :: Loss: 6.373710220941575e-06]\n",
            "Epoch: 26 Label: 1, [Discriminator :: Loss: 4.525542259216309], [ Generator :: Loss: 5.644839802698698e-06]\n",
            "Epoch: 27 Label: 1, [Discriminator :: Loss: 4.639540195465088], [ Generator :: Loss: 5.227240762906149e-06]\n",
            "Epoch: 28 Label: 1, [Discriminator :: Loss: 4.621622562408447], [ Generator :: Loss: 5.536342541745398e-06]\n",
            "Epoch: 29 Label: 1, [Discriminator :: Loss: 4.595691680908203], [ Generator :: Loss: 5.5387404245266225e-06]\n",
            "Epoch: 30 Label: 1, [Discriminator :: Loss: 4.625566482543945], [ Generator :: Loss: 5.682934443029808e-06]\n",
            "Epoch: 31 Label: 1, [Discriminator :: Loss: 4.672222137451172], [ Generator :: Loss: 5.133691502123838e-06]\n",
            "Epoch: 32 Label: 1, [Discriminator :: Loss: 4.72988748550415], [ Generator :: Loss: 4.9207037591258995e-06]\n",
            "Epoch: 33 Label: 1, [Discriminator :: Loss: 4.756436347961426], [ Generator :: Loss: 5.010756012779893e-06]\n",
            "Epoch: 34 Label: 1, [Discriminator :: Loss: 4.764781951904297], [ Generator :: Loss: 4.624216671800241e-06]\n",
            "Epoch: 35 Label: 1, [Discriminator :: Loss: 4.737170219421387], [ Generator :: Loss: 5.090238119009882e-06]\n",
            "Epoch: 36 Label: 1, [Discriminator :: Loss: 4.805050849914551], [ Generator :: Loss: 4.7832172640482895e-06]\n",
            "Epoch: 37 Label: 1, [Discriminator :: Loss: 4.781674861907959], [ Generator :: Loss: 4.6204813770600595e-06]\n",
            "Epoch: 38 Label: 1, [Discriminator :: Loss: 4.860983848571777], [ Generator :: Loss: 4.655768407246796e-06]\n",
            "Epoch: 39 Label: 1, [Discriminator :: Loss: 4.933759689331055], [ Generator :: Loss: 4.7073044697754085e-06]\n",
            "Epoch: 40 Label: 1, [Discriminator :: Loss: 4.978888034820557], [ Generator :: Loss: 4.700574208982289e-06]\n",
            "Epoch: 41 Label: 1, [Discriminator :: Loss: 5.095577716827393], [ Generator :: Loss: 5.09630035594455e-06]\n",
            "Epoch: 42 Label: 1, [Discriminator :: Loss: 5.009286880493164], [ Generator :: Loss: 4.64292998003657e-06]\n",
            "Epoch: 43 Label: 1, [Discriminator :: Loss: 5.025032997131348], [ Generator :: Loss: 4.085717591806315e-06]\n",
            "Epoch: 44 Label: 1, [Discriminator :: Loss: 4.960402965545654], [ Generator :: Loss: 4.485350473260041e-06]\n",
            "Epoch: 45 Label: 1, [Discriminator :: Loss: 4.982189655303955], [ Generator :: Loss: 4.343300133768935e-06]\n",
            "Epoch: 46 Label: 1, [Discriminator :: Loss: 5.115645408630371], [ Generator :: Loss: 4.576407718559494e-06]\n",
            "Epoch: 47 Label: 1, [Discriminator :: Loss: 5.133159637451172], [ Generator :: Loss: 4.3843556341016665e-06]\n",
            "Epoch: 48 Label: 1, [Discriminator :: Loss: 5.084331512451172], [ Generator :: Loss: 4.147018898947863e-06]\n",
            "Epoch: 49 Label: 1, [Discriminator :: Loss: 5.093459129333496], [ Generator :: Loss: 4.4719281504512765e-06]\n",
            "Epoch: 50 Label: 1, [Discriminator :: Loss: 5.100227355957031], [ Generator :: Loss: 4.1053049244510476e-06]\n",
            "Epoch: 51 Label: 1, [Discriminator :: Loss: 5.0548176765441895], [ Generator :: Loss: 3.843316335405689e-06]\n",
            "Epoch: 52 Label: 1, [Discriminator :: Loss: 5.117708206176758], [ Generator :: Loss: 4.056580110045616e-06]\n",
            "Epoch: 53 Label: 1, [Discriminator :: Loss: 5.235783576965332], [ Generator :: Loss: 4.018692379759159e-06]\n",
            "Epoch: 54 Label: 1, [Discriminator :: Loss: 5.242818832397461], [ Generator :: Loss: 3.6304277273302432e-06]\n",
            "Epoch: 55 Label: 1, [Discriminator :: Loss: 5.214054107666016], [ Generator :: Loss: 3.88100625059451e-06]\n",
            "Epoch: 56 Label: 1, [Discriminator :: Loss: 5.235644817352295], [ Generator :: Loss: 3.497852958389558e-06]\n",
            "Epoch: 57 Label: 1, [Discriminator :: Loss: 5.272636413574219], [ Generator :: Loss: 3.656964508991223e-06]\n",
            "Epoch: 58 Label: 1, [Discriminator :: Loss: 5.40713357925415], [ Generator :: Loss: 3.5286643651488703e-06]\n",
            "Epoch: 59 Label: 1, [Discriminator :: Loss: 5.318134307861328], [ Generator :: Loss: 4.182228622084949e-06]\n",
            "Epoch: 60 Label: 1, [Discriminator :: Loss: 5.31149959564209], [ Generator :: Loss: 3.374218749740976e-06]\n",
            "Epoch: 61 Label: 1, [Discriminator :: Loss: 5.374482154846191], [ Generator :: Loss: 3.3393271223758347e-06]\n",
            "Epoch: 62 Label: 1, [Discriminator :: Loss: 5.277676582336426], [ Generator :: Loss: 3.5579448649514234e-06]\n",
            "Epoch: 63 Label: 1, [Discriminator :: Loss: 5.250205039978027], [ Generator :: Loss: 3.5270209082227666e-06]\n",
            "Epoch: 64 Label: 1, [Discriminator :: Loss: 5.417312145233154], [ Generator :: Loss: 3.1826148187974468e-06]\n",
            "Epoch: 65 Label: 1, [Discriminator :: Loss: 5.252294063568115], [ Generator :: Loss: 3.4095583032467403e-06]\n",
            "Epoch: 66 Label: 1, [Discriminator :: Loss: 5.5046868324279785], [ Generator :: Loss: 3.484651642793324e-06]\n",
            "Epoch: 67 Label: 1, [Discriminator :: Loss: 5.415061950683594], [ Generator :: Loss: 3.7312574932002462e-06]\n",
            "Epoch: 68 Label: 1, [Discriminator :: Loss: 5.348870277404785], [ Generator :: Loss: 3.335168912599329e-06]\n",
            "Epoch: 69 Label: 1, [Discriminator :: Loss: 5.349419116973877], [ Generator :: Loss: 3.4834818052331684e-06]\n",
            "Epoch: 70 Label: 1, [Discriminator :: Loss: 5.285745620727539], [ Generator :: Loss: 3.1230097192747053e-06]\n",
            "Epoch: 71 Label: 1, [Discriminator :: Loss: 5.483023643493652], [ Generator :: Loss: 2.9077411909383954e-06]\n",
            "Epoch: 72 Label: 1, [Discriminator :: Loss: 5.561484336853027], [ Generator :: Loss: 3.329746050440008e-06]\n",
            "Epoch: 73 Label: 1, [Discriminator :: Loss: 5.616530895233154], [ Generator :: Loss: 3.0428570880758343e-06]\n",
            "Epoch: 74 Label: 1, [Discriminator :: Loss: 5.59905481338501], [ Generator :: Loss: 3.061227516809595e-06]\n",
            "Epoch: 75 Label: 1, [Discriminator :: Loss: 5.669167995452881], [ Generator :: Loss: 3.0788010008109268e-06]\n",
            "Epoch: 76 Label: 1, [Discriminator :: Loss: 5.597299098968506], [ Generator :: Loss: 3.075531822105404e-06]\n",
            "Epoch: 77 Label: 1, [Discriminator :: Loss: 5.559548377990723], [ Generator :: Loss: 2.9140505830582697e-06]\n",
            "Epoch: 78 Label: 1, [Discriminator :: Loss: 5.638427734375], [ Generator :: Loss: 3.0694850465806667e-06]\n",
            "Epoch: 79 Label: 1, [Discriminator :: Loss: 5.478496551513672], [ Generator :: Loss: 3.124908744212007e-06]\n",
            "Epoch: 80 Label: 1, [Discriminator :: Loss: 5.64109992980957], [ Generator :: Loss: 3.2120742616825737e-06]\n",
            "Epoch: 81 Label: 1, [Discriminator :: Loss: 5.503363132476807], [ Generator :: Loss: 2.689861730686971e-06]\n",
            "Epoch: 82 Label: 1, [Discriminator :: Loss: 5.7703142166137695], [ Generator :: Loss: 2.9153206924092956e-06]\n",
            "Epoch: 83 Label: 1, [Discriminator :: Loss: 5.594264507293701], [ Generator :: Loss: 3.125846887996886e-06]\n",
            "Epoch: 84 Label: 1, [Discriminator :: Loss: 5.500921249389648], [ Generator :: Loss: 2.915984168794239e-06]\n",
            "Epoch: 85 Label: 1, [Discriminator :: Loss: 5.599626541137695], [ Generator :: Loss: 2.7620910714176716e-06]\n",
            "Epoch: 86 Label: 1, [Discriminator :: Loss: 5.77569580078125], [ Generator :: Loss: 3.2365285278501688e-06]\n",
            "Epoch: 87 Label: 1, [Discriminator :: Loss: 5.6856913566589355], [ Generator :: Loss: 2.6492239157960285e-06]\n",
            "Epoch: 88 Label: 1, [Discriminator :: Loss: 5.703706741333008], [ Generator :: Loss: 2.820194595187786e-06]\n",
            "Epoch: 89 Label: 1, [Discriminator :: Loss: 5.680145740509033], [ Generator :: Loss: 2.8971808205824345e-06]\n",
            "Epoch: 90 Label: 1, [Discriminator :: Loss: 5.724241256713867], [ Generator :: Loss: 2.796281023620395e-06]\n",
            "Epoch: 91 Label: 1, [Discriminator :: Loss: 5.6113386154174805], [ Generator :: Loss: 2.5968408863263903e-06]\n",
            "Epoch: 92 Label: 1, [Discriminator :: Loss: 5.685460090637207], [ Generator :: Loss: 2.5354217996209627e-06]\n",
            "Epoch: 93 Label: 1, [Discriminator :: Loss: 5.810971736907959], [ Generator :: Loss: 2.6920683922071476e-06]\n",
            "Epoch: 94 Label: 1, [Discriminator :: Loss: 5.768901824951172], [ Generator :: Loss: 2.4962532734207343e-06]\n",
            "Epoch: 95 Label: 1, [Discriminator :: Loss: 5.683826446533203], [ Generator :: Loss: 2.587111339380499e-06]\n",
            "Epoch: 96 Label: 1, [Discriminator :: Loss: 5.852872848510742], [ Generator :: Loss: 2.5507476948405383e-06]\n",
            "Epoch: 97 Label: 1, [Discriminator :: Loss: 5.778555870056152], [ Generator :: Loss: 2.521869191696169e-06]\n",
            "Epoch: 98 Label: 1, [Discriminator :: Loss: 5.95952033996582], [ Generator :: Loss: 2.4021303488552803e-06]\n",
            "Epoch: 99 Label: 1, [Discriminator :: Loss: 5.795636177062988], [ Generator :: Loss: 2.2641336272499757e-06]\n",
            "Epoch: 100 Label: 1, [Discriminator :: Loss: 5.809169769287109], [ Generator :: Loss: 2.580542513896944e-06]\n",
            "Epoch: 101 Label: 1, [Discriminator :: Loss: 5.86281681060791], [ Generator :: Loss: 2.3028915165923536e-06]\n",
            "Epoch: 102 Label: 1, [Discriminator :: Loss: 5.807286262512207], [ Generator :: Loss: 2.2028871171642095e-06]\n",
            "Epoch: 103 Label: 1, [Discriminator :: Loss: 5.977766990661621], [ Generator :: Loss: 2.4622806904517347e-06]\n",
            "Epoch: 104 Label: 1, [Discriminator :: Loss: 5.904077053070068], [ Generator :: Loss: 2.352232058910886e-06]\n",
            "Epoch: 105 Label: 1, [Discriminator :: Loss: 5.774251937866211], [ Generator :: Loss: 2.316455265827244e-06]\n",
            "Epoch: 106 Label: 1, [Discriminator :: Loss: 6.017078399658203], [ Generator :: Loss: 2.2748381525161676e-06]\n",
            "Epoch: 107 Label: 1, [Discriminator :: Loss: 5.912455081939697], [ Generator :: Loss: 2.4108107936626766e-06]\n",
            "Epoch: 108 Label: 1, [Discriminator :: Loss: 5.806684494018555], [ Generator :: Loss: 2.2826038730272558e-06]\n",
            "Epoch: 109 Label: 1, [Discriminator :: Loss: 5.91880989074707], [ Generator :: Loss: 2.042488631559536e-06]\n",
            "Epoch: 110 Label: 1, [Discriminator :: Loss: 5.977842330932617], [ Generator :: Loss: 2.420416194581776e-06]\n",
            "Epoch: 111 Label: 1, [Discriminator :: Loss: 5.810305595397949], [ Generator :: Loss: 2.0093507373530883e-06]\n",
            "Epoch: 112 Label: 1, [Discriminator :: Loss: 5.769067764282227], [ Generator :: Loss: 2.0254965420463122e-06]\n",
            "Epoch: 113 Label: 1, [Discriminator :: Loss: 5.957535743713379], [ Generator :: Loss: 2.0024890545755625e-06]\n",
            "Epoch: 114 Label: 1, [Discriminator :: Loss: 5.920015335083008], [ Generator :: Loss: 2.005799615290016e-06]\n",
            "Epoch: 115 Label: 1, [Discriminator :: Loss: 6.031346321105957], [ Generator :: Loss: 2.0670831872848794e-06]\n",
            "Epoch: 116 Label: 1, [Discriminator :: Loss: 5.870230674743652], [ Generator :: Loss: 2.106072770402534e-06]\n",
            "Epoch: 117 Label: 1, [Discriminator :: Loss: 5.9213056564331055], [ Generator :: Loss: 1.9551334844436496e-06]\n",
            "Epoch: 118 Label: 1, [Discriminator :: Loss: 5.954712867736816], [ Generator :: Loss: 1.9677349882840645e-06]\n",
            "Epoch: 119 Label: 1, [Discriminator :: Loss: 5.971619129180908], [ Generator :: Loss: 1.9192959825886646e-06]\n",
            "Epoch: 120 Label: 1, [Discriminator :: Loss: 5.928102493286133], [ Generator :: Loss: 1.8332573290535947e-06]\n",
            "Epoch: 121 Label: 1, [Discriminator :: Loss: 5.907037258148193], [ Generator :: Loss: 2.1614703200611984e-06]\n",
            "Epoch: 122 Label: 1, [Discriminator :: Loss: 6.137984275817871], [ Generator :: Loss: 1.831086706260976e-06]\n",
            "Epoch: 123 Label: 1, [Discriminator :: Loss: 5.973001480102539], [ Generator :: Loss: 2.2632707441516686e-06]\n",
            "Epoch: 124 Label: 1, [Discriminator :: Loss: 6.186153411865234], [ Generator :: Loss: 1.9577764760470018e-06]\n",
            "Epoch: 125 Label: 1, [Discriminator :: Loss: 5.970544815063477], [ Generator :: Loss: 2.0243762719474034e-06]\n",
            "Epoch: 126 Label: 1, [Discriminator :: Loss: 6.01348876953125], [ Generator :: Loss: 1.8108744370692875e-06]\n",
            "Epoch: 127 Label: 1, [Discriminator :: Loss: 6.157299995422363], [ Generator :: Loss: 1.9562128272809787e-06]\n",
            "Epoch: 128 Label: 1, [Discriminator :: Loss: 5.883630752563477], [ Generator :: Loss: 1.7601878425921313e-06]\n",
            "Epoch: 129 Label: 1, [Discriminator :: Loss: 5.933183670043945], [ Generator :: Loss: 1.8130440366803668e-06]\n",
            "Epoch: 130 Label: 1, [Discriminator :: Loss: 5.985233306884766], [ Generator :: Loss: 1.6278962675642106e-06]\n",
            "Epoch: 131 Label: 1, [Discriminator :: Loss: 6.043001174926758], [ Generator :: Loss: 2.0743373170262203e-06]\n",
            "Epoch: 132 Label: 1, [Discriminator :: Loss: 5.8844757080078125], [ Generator :: Loss: 2.10326470551081e-06]\n",
            "Epoch: 133 Label: 1, [Discriminator :: Loss: 6.048376560211182], [ Generator :: Loss: 1.701206656434806e-06]\n",
            "Epoch: 134 Label: 1, [Discriminator :: Loss: 6.046648025512695], [ Generator :: Loss: 1.917114332172787e-06]\n",
            "Epoch: 135 Label: 1, [Discriminator :: Loss: 6.0045881271362305], [ Generator :: Loss: 1.7046706943801837e-06]\n",
            "Epoch: 136 Label: 1, [Discriminator :: Loss: 6.059875965118408], [ Generator :: Loss: 1.7975221453525592e-06]\n",
            "Epoch: 137 Label: 1, [Discriminator :: Loss: 6.140722274780273], [ Generator :: Loss: 1.6889014204934938e-06]\n",
            "Epoch: 138 Label: 1, [Discriminator :: Loss: 6.147904396057129], [ Generator :: Loss: 1.982434469027794e-06]\n",
            "Epoch: 139 Label: 1, [Discriminator :: Loss: 6.22537899017334], [ Generator :: Loss: 1.5717314454377629e-06]\n",
            "Epoch: 140 Label: 1, [Discriminator :: Loss: 6.025863170623779], [ Generator :: Loss: 1.818953251131461e-06]\n",
            "Epoch: 141 Label: 1, [Discriminator :: Loss: 6.014935493469238], [ Generator :: Loss: 1.639491301830276e-06]\n",
            "Epoch: 142 Label: 1, [Discriminator :: Loss: 6.17246150970459], [ Generator :: Loss: 1.7092952475650236e-06]\n",
            "Epoch: 143 Label: 1, [Discriminator :: Loss: 6.078975677490234], [ Generator :: Loss: 1.6216160929616308e-06]\n",
            "Epoch: 144 Label: 1, [Discriminator :: Loss: 6.206092834472656], [ Generator :: Loss: 1.9206431716156658e-06]\n",
            "Epoch: 145 Label: 1, [Discriminator :: Loss: 6.0944504737854], [ Generator :: Loss: 1.5314169559133006e-06]\n",
            "Epoch: 146 Label: 1, [Discriminator :: Loss: 6.209658145904541], [ Generator :: Loss: 1.730989538373251e-06]\n",
            "Epoch: 147 Label: 1, [Discriminator :: Loss: 5.967898368835449], [ Generator :: Loss: 1.5336145224864595e-06]\n",
            "Epoch: 148 Label: 1, [Discriminator :: Loss: 6.131954193115234], [ Generator :: Loss: 1.5432026430062251e-06]\n",
            "Epoch: 149 Label: 1, [Discriminator :: Loss: 6.217072486877441], [ Generator :: Loss: 1.6045986512835952e-06]\n",
            "Epoch: 150 Label: 1, [Discriminator :: Loss: 6.070433616638184], [ Generator :: Loss: 1.6003746168280486e-06]\n",
            "Epoch: 151 Label: 1, [Discriminator :: Loss: 6.202517986297607], [ Generator :: Loss: 1.6836163467814913e-06]\n",
            "Epoch: 152 Label: 1, [Discriminator :: Loss: 5.9948225021362305], [ Generator :: Loss: 1.6688848063495243e-06]\n",
            "Epoch: 153 Label: 1, [Discriminator :: Loss: 6.245649337768555], [ Generator :: Loss: 1.6299713934131432e-06]\n",
            "Epoch: 154 Label: 1, [Discriminator :: Loss: 6.205048561096191], [ Generator :: Loss: 1.4201975773175946e-06]\n",
            "Epoch: 155 Label: 1, [Discriminator :: Loss: 6.175759315490723], [ Generator :: Loss: 1.4653580819867784e-06]\n",
            "Epoch: 156 Label: 1, [Discriminator :: Loss: 6.388660430908203], [ Generator :: Loss: 1.5290230521713966e-06]\n",
            "Epoch: 157 Label: 1, [Discriminator :: Loss: 6.310083866119385], [ Generator :: Loss: 1.58721195475664e-06]\n",
            "Epoch: 158 Label: 1, [Discriminator :: Loss: 6.299838066101074], [ Generator :: Loss: 1.772249561327044e-06]\n",
            "Epoch: 159 Label: 1, [Discriminator :: Loss: 6.200453281402588], [ Generator :: Loss: 1.5513244306930574e-06]\n",
            "Epoch: 160 Label: 1, [Discriminator :: Loss: 6.233518123626709], [ Generator :: Loss: 1.4371296401805012e-06]\n",
            "Epoch: 161 Label: 1, [Discriminator :: Loss: 6.1304850578308105], [ Generator :: Loss: 1.5173461633821717e-06]\n",
            "Epoch: 162 Label: 1, [Discriminator :: Loss: 6.40004301071167], [ Generator :: Loss: 1.3752036238656729e-06]\n",
            "Epoch: 163 Label: 1, [Discriminator :: Loss: 6.125472068786621], [ Generator :: Loss: 1.3910998859500978e-06]\n",
            "Epoch: 164 Label: 1, [Discriminator :: Loss: 6.21097469329834], [ Generator :: Loss: 1.3933747595729074e-06]\n",
            "Epoch: 165 Label: 1, [Discriminator :: Loss: 6.217720985412598], [ Generator :: Loss: 1.4384551150214975e-06]\n",
            "Epoch: 166 Label: 1, [Discriminator :: Loss: 6.220099449157715], [ Generator :: Loss: 1.5483526567550143e-06]\n",
            "Epoch: 167 Label: 1, [Discriminator :: Loss: 6.329794883728027], [ Generator :: Loss: 1.559255792926706e-06]\n",
            "Epoch: 168 Label: 1, [Discriminator :: Loss: 6.20806884765625], [ Generator :: Loss: 1.3818987554259365e-06]\n",
            "Epoch: 169 Label: 1, [Discriminator :: Loss: 6.189741134643555], [ Generator :: Loss: 1.3414509112408268e-06]\n",
            "Epoch: 170 Label: 1, [Discriminator :: Loss: 6.138422012329102], [ Generator :: Loss: 1.4424687151404214e-06]\n",
            "Epoch: 171 Label: 1, [Discriminator :: Loss: 6.26141357421875], [ Generator :: Loss: 1.353375978396798e-06]\n",
            "Epoch: 172 Label: 1, [Discriminator :: Loss: 6.387809753417969], [ Generator :: Loss: 1.3795809081784682e-06]\n",
            "Epoch: 173 Label: 1, [Discriminator :: Loss: 6.202001571655273], [ Generator :: Loss: 1.433943907613866e-06]\n",
            "Epoch: 174 Label: 1, [Discriminator :: Loss: 6.256473064422607], [ Generator :: Loss: 1.3649685115524335e-06]\n",
            "Epoch: 175 Label: 1, [Discriminator :: Loss: 6.063990592956543], [ Generator :: Loss: 1.394729906678549e-06]\n",
            "Epoch: 176 Label: 1, [Discriminator :: Loss: 6.344799041748047], [ Generator :: Loss: 1.2960811091033975e-06]\n",
            "Epoch: 177 Label: 1, [Discriminator :: Loss: 6.269254684448242], [ Generator :: Loss: 1.3731519175053108e-06]\n",
            "Epoch: 178 Label: 1, [Discriminator :: Loss: 6.193416595458984], [ Generator :: Loss: 1.4247307262849063e-06]\n",
            "Epoch: 179 Label: 1, [Discriminator :: Loss: 6.333731174468994], [ Generator :: Loss: 1.2590267033374403e-06]\n",
            "Epoch: 180 Label: 1, [Discriminator :: Loss: 6.325894355773926], [ Generator :: Loss: 1.2910495570395142e-06]\n",
            "Epoch: 181 Label: 1, [Discriminator :: Loss: 6.246244430541992], [ Generator :: Loss: 1.3029905403527664e-06]\n",
            "Epoch: 182 Label: 1, [Discriminator :: Loss: 6.181333541870117], [ Generator :: Loss: 1.239707899003406e-06]\n",
            "Epoch: 183 Label: 1, [Discriminator :: Loss: 6.318612575531006], [ Generator :: Loss: 1.3821620541420998e-06]\n",
            "Epoch: 184 Label: 1, [Discriminator :: Loss: 6.255695343017578], [ Generator :: Loss: 1.3258926401249482e-06]\n",
            "Epoch: 185 Label: 1, [Discriminator :: Loss: 6.19179630279541], [ Generator :: Loss: 1.3532205684896326e-06]\n",
            "Epoch: 186 Label: 1, [Discriminator :: Loss: 6.254748344421387], [ Generator :: Loss: 1.209155698234099e-06]\n",
            "Epoch: 187 Label: 1, [Discriminator :: Loss: 6.324761867523193], [ Generator :: Loss: 1.2317793789407006e-06]\n",
            "Epoch: 188 Label: 1, [Discriminator :: Loss: 6.240317344665527], [ Generator :: Loss: 1.1401341453165514e-06]\n",
            "Epoch: 189 Label: 1, [Discriminator :: Loss: 6.342945098876953], [ Generator :: Loss: 1.314014980380307e-06]\n",
            "Epoch: 190 Label: 1, [Discriminator :: Loss: 6.226807594299316], [ Generator :: Loss: 1.2366524515528e-06]\n",
            "Epoch: 191 Label: 1, [Discriminator :: Loss: 6.349055290222168], [ Generator :: Loss: 1.1582045544855646e-06]\n",
            "Epoch: 192 Label: 1, [Discriminator :: Loss: 6.347075462341309], [ Generator :: Loss: 1.3982246400701115e-06]\n",
            "Epoch: 193 Label: 1, [Discriminator :: Loss: 6.220544338226318], [ Generator :: Loss: 1.2794306485375273e-06]\n",
            "Epoch: 194 Label: 1, [Discriminator :: Loss: 6.325241565704346], [ Generator :: Loss: 1.2986763522349065e-06]\n",
            "Epoch: 195 Label: 1, [Discriminator :: Loss: 6.336850166320801], [ Generator :: Loss: 1.1646197890513577e-06]\n",
            "Epoch: 196 Label: 1, [Discriminator :: Loss: 6.236950874328613], [ Generator :: Loss: 1.265546188733424e-06]\n",
            "Epoch: 197 Label: 1, [Discriminator :: Loss: 6.219361782073975], [ Generator :: Loss: 1.136008449975634e-06]\n",
            "Epoch: 198 Label: 1, [Discriminator :: Loss: 6.315434455871582], [ Generator :: Loss: 1.226496806339128e-06]\n",
            "Epoch: 199 Label: 1, [Discriminator :: Loss: 6.183000564575195], [ Generator :: Loss: 1.104804823626182e-06]\n",
            "Epoch: 200 Label: 1, [Discriminator :: Loss: 6.26068639755249], [ Generator :: Loss: 1.1254774108238053e-06]\n",
            "Epoch: 201 Label: 1, [Discriminator :: Loss: 6.265880584716797], [ Generator :: Loss: 1.2336690815573093e-06]\n",
            "Epoch: 202 Label: 1, [Discriminator :: Loss: 6.258825302124023], [ Generator :: Loss: 1.1332174381095683e-06]\n",
            "Epoch: 203 Label: 1, [Discriminator :: Loss: 6.319787979125977], [ Generator :: Loss: 1.1924703358090483e-06]\n",
            "Epoch: 204 Label: 1, [Discriminator :: Loss: 6.311004638671875], [ Generator :: Loss: 1.3080872349746642e-06]\n",
            "Epoch: 205 Label: 1, [Discriminator :: Loss: 6.339285850524902], [ Generator :: Loss: 1.1818267466878751e-06]\n",
            "Epoch: 206 Label: 1, [Discriminator :: Loss: 6.314729690551758], [ Generator :: Loss: 1.1453021215857007e-06]\n",
            "Epoch: 207 Label: 1, [Discriminator :: Loss: 6.427179336547852], [ Generator :: Loss: 1.3519035064746276e-06]\n",
            "Epoch: 208 Label: 1, [Discriminator :: Loss: 6.425010681152344], [ Generator :: Loss: 1.2520446261987672e-06]\n",
            "Epoch: 209 Label: 1, [Discriminator :: Loss: 6.350385665893555], [ Generator :: Loss: 1.1121501302113757e-06]\n",
            "Epoch: 210 Label: 1, [Discriminator :: Loss: 6.273818492889404], [ Generator :: Loss: 1.1328033906465862e-06]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:128: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 211 Label: 1, [Discriminator :: Loss: 6.478871822357178], [ Generator :: Loss: 1.1065010312449886e-06]\n",
            "Epoch: 212 Label: 1, [Discriminator :: Loss: 6.216343879699707], [ Generator :: Loss: 1.1179904504388105e-06]\n",
            "Epoch: 213 Label: 1, [Discriminator :: Loss: 6.395967483520508], [ Generator :: Loss: 1.112848281081824e-06]\n",
            "Epoch: 214 Label: 1, [Discriminator :: Loss: 6.310398101806641], [ Generator :: Loss: 1.0910425771726295e-06]\n",
            "Epoch: 215 Label: 1, [Discriminator :: Loss: 6.450102806091309], [ Generator :: Loss: 1.0531235830057994e-06]\n",
            "Epoch: 216 Label: 1, [Discriminator :: Loss: 6.2627482414245605], [ Generator :: Loss: 1.2085782827853109e-06]\n",
            "Epoch: 217 Label: 1, [Discriminator :: Loss: 6.424884796142578], [ Generator :: Loss: 1.0435090871396824e-06]\n",
            "Epoch: 218 Label: 1, [Discriminator :: Loss: 6.340157985687256], [ Generator :: Loss: 1.0456370773681556e-06]\n",
            "Epoch: 219 Label: 1, [Discriminator :: Loss: 6.198159694671631], [ Generator :: Loss: 1.1211326409465983e-06]\n",
            "Epoch: 220 Label: 1, [Discriminator :: Loss: 6.246676445007324], [ Generator :: Loss: 1.0614958227961324e-06]\n",
            "Epoch: 221 Label: 1, [Discriminator :: Loss: 6.375689506530762], [ Generator :: Loss: 9.677264642959926e-07]\n",
            "Epoch: 222 Label: 1, [Discriminator :: Loss: 6.364295959472656], [ Generator :: Loss: 9.917655461322283e-07]\n",
            "Epoch: 223 Label: 1, [Discriminator :: Loss: 6.403594970703125], [ Generator :: Loss: 1.1372501376172295e-06]\n",
            "Epoch: 224 Label: 1, [Discriminator :: Loss: 6.396703720092773], [ Generator :: Loss: 1.1709494174283464e-06]\n",
            "Epoch: 225 Label: 1, [Discriminator :: Loss: 6.343157768249512], [ Generator :: Loss: 1.0272289046042715e-06]\n",
            "Epoch: 226 Label: 1, [Discriminator :: Loss: 6.510443687438965], [ Generator :: Loss: 1.1418694612075342e-06]\n",
            "Epoch: 227 Label: 1, [Discriminator :: Loss: 6.3191986083984375], [ Generator :: Loss: 9.777005516298232e-07]\n",
            "Epoch: 228 Label: 1, [Discriminator :: Loss: 6.371448516845703], [ Generator :: Loss: 9.555351425660774e-07]\n",
            "Epoch: 229 Label: 1, [Discriminator :: Loss: 6.344037055969238], [ Generator :: Loss: 1.0359981388319284e-06]\n",
            "Epoch: 230 Label: 1, [Discriminator :: Loss: 6.3437652587890625], [ Generator :: Loss: 1.11283202386403e-06]\n",
            "Epoch: 231 Label: 1, [Discriminator :: Loss: 6.366149425506592], [ Generator :: Loss: 1.0661665328370873e-06]\n",
            "Epoch: 232 Label: 1, [Discriminator :: Loss: 6.410791397094727], [ Generator :: Loss: 1.032152113111806e-06]\n",
            "Epoch: 233 Label: 1, [Discriminator :: Loss: 6.395110607147217], [ Generator :: Loss: 1.068519736691087e-06]\n",
            "Epoch: 234 Label: 1, [Discriminator :: Loss: 6.468513011932373], [ Generator :: Loss: 9.9812746157113e-07]\n",
            "Epoch: 235 Label: 1, [Discriminator :: Loss: 6.369671821594238], [ Generator :: Loss: 9.463511787544121e-07]\n",
            "Epoch: 236 Label: 1, [Discriminator :: Loss: 6.400982856750488], [ Generator :: Loss: 1.0173343980568461e-06]\n",
            "Epoch: 237 Label: 1, [Discriminator :: Loss: 6.381758689880371], [ Generator :: Loss: 1.010197138384683e-06]\n",
            "Epoch: 238 Label: 1, [Discriminator :: Loss: 6.328207015991211], [ Generator :: Loss: 9.223496704180434e-07]\n",
            "Epoch: 239 Label: 1, [Discriminator :: Loss: 6.458024978637695], [ Generator :: Loss: 1.06040067748836e-06]\n",
            "Epoch: 240 Label: 1, [Discriminator :: Loss: 6.463385105133057], [ Generator :: Loss: 1.0054526455860469e-06]\n",
            "Epoch: 241 Label: 1, [Discriminator :: Loss: 6.22721529006958], [ Generator :: Loss: 9.419968023394176e-07]\n",
            "Epoch: 242 Label: 1, [Discriminator :: Loss: 6.412572860717773], [ Generator :: Loss: 9.423437177247251e-07]\n",
            "Epoch: 243 Label: 1, [Discriminator :: Loss: 6.406098365783691], [ Generator :: Loss: 1.0245380508422386e-06]\n",
            "Epoch: 244 Label: 1, [Discriminator :: Loss: 6.374821662902832], [ Generator :: Loss: 1.0398271115263924e-06]\n",
            "Epoch: 245 Label: 1, [Discriminator :: Loss: 6.330620765686035], [ Generator :: Loss: 9.478105766902445e-07]\n",
            "Epoch: 246 Label: 1, [Discriminator :: Loss: 6.397674560546875], [ Generator :: Loss: 9.403256058249099e-07]\n",
            "Epoch: 247 Label: 1, [Discriminator :: Loss: 6.351912498474121], [ Generator :: Loss: 9.765907407199848e-07]\n",
            "Epoch: 248 Label: 1, [Discriminator :: Loss: 6.504567623138428], [ Generator :: Loss: 9.979507922253106e-07]\n",
            "Epoch: 249 Label: 1, [Discriminator :: Loss: 6.335013389587402], [ Generator :: Loss: 9.669018936619977e-07]\n",
            "Epoch: 250 Label: 1, [Discriminator :: Loss: 6.396607398986816], [ Generator :: Loss: 1.0015743328040116e-06]\n",
            "Epoch: 251 Label: 1, [Discriminator :: Loss: 6.479362964630127], [ Generator :: Loss: 1.007431592370267e-06]\n",
            "Epoch: 252 Label: 1, [Discriminator :: Loss: 6.3349504470825195], [ Generator :: Loss: 9.611967470846139e-07]\n",
            "Epoch: 253 Label: 1, [Discriminator :: Loss: 6.364250183105469], [ Generator :: Loss: 9.804717819861253e-07]\n",
            "Epoch: 254 Label: 1, [Discriminator :: Loss: 6.252195358276367], [ Generator :: Loss: 9.77720219452749e-07]\n",
            "Epoch: 255 Label: 1, [Discriminator :: Loss: 6.634360313415527], [ Generator :: Loss: 8.501874617650174e-07]\n",
            "Epoch: 256 Label: 1, [Discriminator :: Loss: 6.435112476348877], [ Generator :: Loss: 8.805267839306907e-07]\n",
            "Epoch: 257 Label: 1, [Discriminator :: Loss: 6.363081932067871], [ Generator :: Loss: 9.137610845755262e-07]\n",
            "Epoch: 258 Label: 1, [Discriminator :: Loss: 6.380036354064941], [ Generator :: Loss: 8.974625416158233e-07]\n",
            "Epoch: 259 Label: 1, [Discriminator :: Loss: 6.396027088165283], [ Generator :: Loss: 8.918254934542347e-07]\n",
            "Epoch: 260 Label: 1, [Discriminator :: Loss: 6.468958377838135], [ Generator :: Loss: 1.0607977856125217e-06]\n",
            "Epoch: 261 Label: 1, [Discriminator :: Loss: 6.457297325134277], [ Generator :: Loss: 8.668978352943668e-07]\n",
            "Epoch: 262 Label: 1, [Discriminator :: Loss: 6.257305145263672], [ Generator :: Loss: 8.570948466513073e-07]\n",
            "Epoch: 263 Label: 1, [Discriminator :: Loss: 6.266822814941406], [ Generator :: Loss: 8.311716896969301e-07]\n",
            "Epoch: 264 Label: 1, [Discriminator :: Loss: 6.26010799407959], [ Generator :: Loss: 8.783674161350064e-07]\n",
            "Epoch: 265 Label: 1, [Discriminator :: Loss: 6.499090194702148], [ Generator :: Loss: 9.423485494153283e-07]\n",
            "Epoch: 266 Label: 1, [Discriminator :: Loss: 6.278968811035156], [ Generator :: Loss: 9.942738188328804e-07]\n",
            "Epoch: 267 Label: 1, [Discriminator :: Loss: 6.488670349121094], [ Generator :: Loss: 8.549754966225009e-07]\n",
            "Epoch: 268 Label: 1, [Discriminator :: Loss: 6.364935874938965], [ Generator :: Loss: 8.432373306277441e-07]\n",
            "Epoch: 269 Label: 1, [Discriminator :: Loss: 6.4780426025390625], [ Generator :: Loss: 8.910147926144418e-07]\n",
            "Epoch: 270 Label: 1, [Discriminator :: Loss: 6.404786109924316], [ Generator :: Loss: 8.916279625736934e-07]\n",
            "Epoch: 271 Label: 1, [Discriminator :: Loss: 6.41187858581543], [ Generator :: Loss: 8.796748716122238e-07]\n",
            "Epoch: 272 Label: 1, [Discriminator :: Loss: 6.463853359222412], [ Generator :: Loss: 8.432368758803932e-07]\n",
            "Epoch: 273 Label: 1, [Discriminator :: Loss: 6.436724662780762], [ Generator :: Loss: 8.952298458098085e-07]\n",
            "Epoch: 274 Label: 1, [Discriminator :: Loss: 6.331491470336914], [ Generator :: Loss: 8.685190664436959e-07]\n",
            "Epoch: 275 Label: 1, [Discriminator :: Loss: 6.25510311126709], [ Generator :: Loss: 9.152499842457473e-07]\n",
            "Epoch: 276 Label: 1, [Discriminator :: Loss: 6.409235954284668], [ Generator :: Loss: 8.88390445652476e-07]\n",
            "Epoch: 277 Label: 1, [Discriminator :: Loss: 6.438672065734863], [ Generator :: Loss: 7.773448942316463e-07]\n",
            "Epoch: 278 Label: 1, [Discriminator :: Loss: 6.449184417724609], [ Generator :: Loss: 8.780639859651274e-07]\n",
            "Epoch: 279 Label: 1, [Discriminator :: Loss: 6.413543701171875], [ Generator :: Loss: 8.375380957659218e-07]\n",
            "Epoch: 280 Label: 1, [Discriminator :: Loss: 6.225634574890137], [ Generator :: Loss: 8.805190532257257e-07]\n",
            "Epoch: 281 Label: 1, [Discriminator :: Loss: 6.456888675689697], [ Generator :: Loss: 8.963623940871912e-07]\n",
            "Epoch: 282 Label: 1, [Discriminator :: Loss: 6.390185356140137], [ Generator :: Loss: 8.404443860854371e-07]\n",
            "Epoch: 283 Label: 1, [Discriminator :: Loss: 6.510154724121094], [ Generator :: Loss: 8.76981516739761e-07]\n",
            "Epoch: 284 Label: 1, [Discriminator :: Loss: 6.354520320892334], [ Generator :: Loss: 8.023116606636904e-07]\n",
            "Epoch: 285 Label: 1, [Discriminator :: Loss: 6.336752891540527], [ Generator :: Loss: 8.561357276448689e-07]\n",
            "Epoch: 286 Label: 1, [Discriminator :: Loss: 6.609764099121094], [ Generator :: Loss: 9.158311513601802e-07]\n",
            "Epoch: 287 Label: 1, [Discriminator :: Loss: 6.435074806213379], [ Generator :: Loss: 7.846575726944138e-07]\n",
            "Epoch: 288 Label: 1, [Discriminator :: Loss: 6.481922149658203], [ Generator :: Loss: 8.265028554887976e-07]\n",
            "Epoch: 289 Label: 1, [Discriminator :: Loss: 6.230203628540039], [ Generator :: Loss: 7.401148423014092e-07]\n",
            "Epoch: 290 Label: 1, [Discriminator :: Loss: 6.47900390625], [ Generator :: Loss: 8.157405773090431e-07]\n",
            "Epoch: 291 Label: 1, [Discriminator :: Loss: 6.454442024230957], [ Generator :: Loss: 8.529128194822988e-07]\n",
            "Epoch: 292 Label: 1, [Discriminator :: Loss: 6.2594122886657715], [ Generator :: Loss: 7.504216341658321e-07]\n",
            "Epoch: 293 Label: 1, [Discriminator :: Loss: 6.545677661895752], [ Generator :: Loss: 7.516144364672073e-07]\n",
            "Epoch: 294 Label: 1, [Discriminator :: Loss: 6.499826431274414], [ Generator :: Loss: 9.122495612245984e-07]\n",
            "Epoch: 295 Label: 1, [Discriminator :: Loss: 6.367524147033691], [ Generator :: Loss: 8.120625807350734e-07]\n",
            "Epoch: 296 Label: 1, [Discriminator :: Loss: 6.4223551750183105], [ Generator :: Loss: 8.385865157833905e-07]\n",
            "Epoch: 297 Label: 1, [Discriminator :: Loss: 6.3660173416137695], [ Generator :: Loss: 9.544427257424104e-07]\n",
            "Epoch: 298 Label: 1, [Discriminator :: Loss: 6.318953514099121], [ Generator :: Loss: 8.243096658588911e-07]\n",
            "Epoch: 299 Label: 1, [Discriminator :: Loss: 6.304434299468994], [ Generator :: Loss: 8.800160458122264e-07]\n",
            "Epoch: 300 Label: 1, [Discriminator :: Loss: 6.411612033843994], [ Generator :: Loss: 7.478836892005347e-07]\n",
            "Epoch: 301 Label: 1, [Discriminator :: Loss: 6.315261363983154], [ Generator :: Loss: 7.640420562893269e-07]\n",
            "Epoch: 302 Label: 1, [Discriminator :: Loss: 6.449747085571289], [ Generator :: Loss: 7.710357294854475e-07]\n",
            "Epoch: 303 Label: 1, [Discriminator :: Loss: 6.25701904296875], [ Generator :: Loss: 7.402966275549261e-07]\n",
            "Epoch: 304 Label: 1, [Discriminator :: Loss: 6.463747978210449], [ Generator :: Loss: 7.663800261070719e-07]\n",
            "Epoch: 305 Label: 1, [Discriminator :: Loss: 6.389050006866455], [ Generator :: Loss: 7.609212389070308e-07]\n",
            "Epoch: 306 Label: 1, [Discriminator :: Loss: 6.426909923553467], [ Generator :: Loss: 7.484268849111686e-07]\n",
            "Epoch: 307 Label: 1, [Discriminator :: Loss: 6.39863395690918], [ Generator :: Loss: 7.680065436943551e-07]\n",
            "Epoch: 308 Label: 1, [Discriminator :: Loss: 6.432600975036621], [ Generator :: Loss: 7.404710800074099e-07]\n",
            "Epoch: 309 Label: 1, [Discriminator :: Loss: 6.53162956237793], [ Generator :: Loss: 7.522274927396211e-07]\n",
            "Epoch: 310 Label: 1, [Discriminator :: Loss: 6.381746768951416], [ Generator :: Loss: 8.201173500310688e-07]\n",
            "Epoch: 311 Label: 1, [Discriminator :: Loss: 6.414431095123291], [ Generator :: Loss: 7.980045211297693e-07]\n",
            "Epoch: 312 Label: 1, [Discriminator :: Loss: 6.362505912780762], [ Generator :: Loss: 7.843522098482936e-07]\n",
            "Epoch: 313 Label: 1, [Discriminator :: Loss: 6.532310485839844], [ Generator :: Loss: 7.821453209544416e-07]\n",
            "Epoch: 314 Label: 1, [Discriminator :: Loss: 6.561568737030029], [ Generator :: Loss: 7.600465323776007e-07]\n",
            "Epoch: 315 Label: 1, [Discriminator :: Loss: 6.3001790046691895], [ Generator :: Loss: 6.997307195888425e-07]\n",
            "Epoch: 316 Label: 1, [Discriminator :: Loss: 6.431591033935547], [ Generator :: Loss: 7.674526614209753e-07]\n",
            "Epoch: 317 Label: 1, [Discriminator :: Loss: 6.36688232421875], [ Generator :: Loss: 7.904651511125849e-07]\n",
            "Epoch: 318 Label: 1, [Discriminator :: Loss: 6.338399887084961], [ Generator :: Loss: 7.20377045126952e-07]\n",
            "Epoch: 319 Label: 1, [Discriminator :: Loss: 6.393929958343506], [ Generator :: Loss: 7.577865517305327e-07]\n",
            "Epoch: 320 Label: 1, [Discriminator :: Loss: 6.431212425231934], [ Generator :: Loss: 7.026380899333162e-07]\n",
            "Epoch: 321 Label: 1, [Discriminator :: Loss: 6.47607421875], [ Generator :: Loss: 7.079290185174614e-07]\n",
            "Epoch: 322 Label: 1, [Discriminator :: Loss: 6.443836688995361], [ Generator :: Loss: 7.094365628290689e-07]\n",
            "Epoch: 323 Label: 1, [Discriminator :: Loss: 6.211180686950684], [ Generator :: Loss: 7.174958795985731e-07]\n",
            "Epoch: 324 Label: 1, [Discriminator :: Loss: 6.321197509765625], [ Generator :: Loss: 7.233101086967508e-07]\n",
            "Epoch: 325 Label: 1, [Discriminator :: Loss: 6.288723468780518], [ Generator :: Loss: 8.120426855384721e-07]\n",
            "Epoch: 326 Label: 1, [Discriminator :: Loss: 6.458728790283203], [ Generator :: Loss: 7.596530622322462e-07]\n",
            "Epoch: 327 Label: 1, [Discriminator :: Loss: 6.453878879547119], [ Generator :: Loss: 7.24657297723752e-07]\n",
            "Epoch: 328 Label: 1, [Discriminator :: Loss: 6.2899627685546875], [ Generator :: Loss: 6.954106765988399e-07]\n",
            "Epoch: 329 Label: 1, [Discriminator :: Loss: 6.426624774932861], [ Generator :: Loss: 7.45100066978921e-07]\n",
            "Epoch: 330 Label: 1, [Discriminator :: Loss: 6.50385856628418], [ Generator :: Loss: 8.193010785362276e-07]\n",
            "Epoch: 331 Label: 1, [Discriminator :: Loss: 6.3676605224609375], [ Generator :: Loss: 8.106903237603547e-07]\n",
            "Epoch: 332 Label: 1, [Discriminator :: Loss: 6.348092555999756], [ Generator :: Loss: 6.910329375386937e-07]\n",
            "Epoch: 333 Label: 1, [Discriminator :: Loss: 6.4264817237854], [ Generator :: Loss: 8.296442501887213e-07]\n",
            "Epoch: 334 Label: 1, [Discriminator :: Loss: 6.383289337158203], [ Generator :: Loss: 6.843532673883601e-07]\n",
            "Epoch: 335 Label: 1, [Discriminator :: Loss: 6.366972923278809], [ Generator :: Loss: 6.652842330368003e-07]\n",
            "Epoch: 336 Label: 1, [Discriminator :: Loss: 6.471738815307617], [ Generator :: Loss: 7.419204166581039e-07]\n",
            "Epoch: 337 Label: 1, [Discriminator :: Loss: 6.414469242095947], [ Generator :: Loss: 7.38187395654677e-07]\n",
            "Epoch: 338 Label: 1, [Discriminator :: Loss: 6.38259220123291], [ Generator :: Loss: 6.587262078028289e-07]\n",
            "Epoch: 339 Label: 1, [Discriminator :: Loss: 6.429125785827637], [ Generator :: Loss: 7.847965548535285e-07]\n",
            "Epoch: 340 Label: 1, [Discriminator :: Loss: 6.341178894042969], [ Generator :: Loss: 8.064083658609889e-07]\n",
            "Epoch: 341 Label: 1, [Discriminator :: Loss: 6.344367980957031], [ Generator :: Loss: 6.688503049190331e-07]\n",
            "Epoch: 342 Label: 1, [Discriminator :: Loss: 6.335968971252441], [ Generator :: Loss: 7.114628601812001e-07]\n",
            "Epoch: 343 Label: 1, [Discriminator :: Loss: 6.344735145568848], [ Generator :: Loss: 7.894894338278391e-07]\n",
            "Epoch: 344 Label: 1, [Discriminator :: Loss: 6.188077449798584], [ Generator :: Loss: 6.52141011414642e-07]\n",
            "Epoch: 345 Label: 1, [Discriminator :: Loss: 6.437958717346191], [ Generator :: Loss: 6.782411219319329e-07]\n",
            "Epoch: 346 Label: 1, [Discriminator :: Loss: 6.316162109375], [ Generator :: Loss: 7.185545314314368e-07]\n",
            "Epoch: 347 Label: 1, [Discriminator :: Loss: 6.511783599853516], [ Generator :: Loss: 6.341823564071092e-07]\n",
            "Epoch: 348 Label: 1, [Discriminator :: Loss: 6.433300018310547], [ Generator :: Loss: 6.831622272329696e-07]\n",
            "Epoch: 349 Label: 1, [Discriminator :: Loss: 6.387846946716309], [ Generator :: Loss: 6.255360176510294e-07]\n",
            "Epoch: 350 Label: 1, [Discriminator :: Loss: 6.443259239196777], [ Generator :: Loss: 6.691025760119373e-07]\n",
            "Epoch: 351 Label: 1, [Discriminator :: Loss: 6.36093282699585], [ Generator :: Loss: 6.836636430307408e-07]\n",
            "Epoch: 352 Label: 1, [Discriminator :: Loss: 6.435489654541016], [ Generator :: Loss: 6.930745826139173e-07]\n",
            "Epoch: 353 Label: 1, [Discriminator :: Loss: 6.454378128051758], [ Generator :: Loss: 7.292802592928638e-07]\n",
            "Epoch: 354 Label: 1, [Discriminator :: Loss: 6.458291530609131], [ Generator :: Loss: 6.54700727409363e-07]\n",
            "Epoch: 355 Label: 1, [Discriminator :: Loss: 6.393332481384277], [ Generator :: Loss: 6.743583185198077e-07]\n",
            "Epoch: 356 Label: 1, [Discriminator :: Loss: 6.292645454406738], [ Generator :: Loss: 6.494026365544414e-07]\n",
            "Epoch: 357 Label: 1, [Discriminator :: Loss: 6.277667045593262], [ Generator :: Loss: 6.16171973888413e-07]\n",
            "Epoch: 358 Label: 1, [Discriminator :: Loss: 6.407265663146973], [ Generator :: Loss: 6.311433367045538e-07]\n",
            "Epoch: 359 Label: 1, [Discriminator :: Loss: 6.2812700271606445], [ Generator :: Loss: 7.711669809395971e-07]\n",
            "Epoch: 360 Label: 1, [Discriminator :: Loss: 6.2849626541137695], [ Generator :: Loss: 6.472270115409628e-07]\n",
            "Epoch: 361 Label: 1, [Discriminator :: Loss: 6.374948501586914], [ Generator :: Loss: 6.643500682912418e-07]\n",
            "Epoch: 362 Label: 1, [Discriminator :: Loss: 6.40834903717041], [ Generator :: Loss: 6.528570111186127e-07]\n",
            "Epoch: 363 Label: 1, [Discriminator :: Loss: 6.367352485656738], [ Generator :: Loss: 7.159383699217869e-07]\n",
            "Epoch: 364 Label: 1, [Discriminator :: Loss: 6.5085883140563965], [ Generator :: Loss: 7.253054263856029e-07]\n",
            "Epoch: 365 Label: 1, [Discriminator :: Loss: 6.410964488983154], [ Generator :: Loss: 6.650122941209702e-07]\n",
            "Epoch: 366 Label: 1, [Discriminator :: Loss: 6.460771560668945], [ Generator :: Loss: 6.263093723646307e-07]\n",
            "Epoch: 367 Label: 1, [Discriminator :: Loss: 6.375496864318848], [ Generator :: Loss: 6.644299901381601e-07]\n",
            "Epoch: 368 Label: 1, [Discriminator :: Loss: 6.155936241149902], [ Generator :: Loss: 6.672818244624068e-07]\n",
            "Epoch: 369 Label: 1, [Discriminator :: Loss: 6.37060546875], [ Generator :: Loss: 6.508136607408233e-07]\n",
            "Epoch: 370 Label: 1, [Discriminator :: Loss: 6.407564163208008], [ Generator :: Loss: 7.164300086515141e-07]\n",
            "Epoch: 371 Label: 1, [Discriminator :: Loss: 6.460946559906006], [ Generator :: Loss: 6.276645763136912e-07]\n",
            "Epoch: 372 Label: 1, [Discriminator :: Loss: 6.340269088745117], [ Generator :: Loss: 6.645300345553551e-07]\n",
            "Epoch: 373 Label: 1, [Discriminator :: Loss: 6.32781457901001], [ Generator :: Loss: 6.959462552913465e-07]\n",
            "Epoch: 374 Label: 1, [Discriminator :: Loss: 6.4200239181518555], [ Generator :: Loss: 6.21973072156834e-07]\n",
            "Epoch: 375 Label: 1, [Discriminator :: Loss: 6.25595235824585], [ Generator :: Loss: 5.960469025012571e-07]\n",
            "Epoch: 376 Label: 1, [Discriminator :: Loss: 6.367432594299316], [ Generator :: Loss: 6.470065727626206e-07]\n",
            "Epoch: 377 Label: 1, [Discriminator :: Loss: 6.364148139953613], [ Generator :: Loss: 6.07964352639101e-07]\n",
            "Epoch: 378 Label: 1, [Discriminator :: Loss: 6.2636566162109375], [ Generator :: Loss: 6.101285521253885e-07]\n",
            "Epoch: 379 Label: 1, [Discriminator :: Loss: 6.360019683837891], [ Generator :: Loss: 6.587196708096599e-07]\n",
            "Epoch: 380 Label: 1, [Discriminator :: Loss: 6.39216423034668], [ Generator :: Loss: 7.030381539152586e-07]\n",
            "Epoch: 381 Label: 1, [Discriminator :: Loss: 6.345761775970459], [ Generator :: Loss: 7.345179255935363e-07]\n",
            "Epoch: 382 Label: 1, [Discriminator :: Loss: 6.402255058288574], [ Generator :: Loss: 6.816705990786431e-07]\n",
            "Epoch: 383 Label: 1, [Discriminator :: Loss: 6.318728446960449], [ Generator :: Loss: 6.574701956196805e-07]\n",
            "Epoch: 384 Label: 1, [Discriminator :: Loss: 6.435229301452637], [ Generator :: Loss: 6.454565095737053e-07]\n",
            "Epoch: 385 Label: 1, [Discriminator :: Loss: 6.362274169921875], [ Generator :: Loss: 6.391666147465003e-07]\n",
            "Epoch: 386 Label: 1, [Discriminator :: Loss: 6.421885013580322], [ Generator :: Loss: 6.214436325535644e-07]\n",
            "Epoch: 387 Label: 1, [Discriminator :: Loss: 6.435885429382324], [ Generator :: Loss: 6.200117468324606e-07]\n",
            "Epoch: 388 Label: 1, [Discriminator :: Loss: 6.3867950439453125], [ Generator :: Loss: 6.289118914537539e-07]\n",
            "Epoch: 389 Label: 1, [Discriminator :: Loss: 6.4400739669799805], [ Generator :: Loss: 6.205796125868801e-07]\n",
            "Epoch: 390 Label: 1, [Discriminator :: Loss: 6.2822771072387695], [ Generator :: Loss: 6.15878775533929e-07]\n",
            "Epoch: 391 Label: 1, [Discriminator :: Loss: 6.383090019226074], [ Generator :: Loss: 6.094260243116878e-07]\n",
            "Epoch: 392 Label: 1, [Discriminator :: Loss: 6.456595420837402], [ Generator :: Loss: 5.925791697336535e-07]\n",
            "Epoch: 393 Label: 1, [Discriminator :: Loss: 6.34457540512085], [ Generator :: Loss: 5.775148679276754e-07]\n",
            "Epoch: 394 Label: 1, [Discriminator :: Loss: 6.508874893188477], [ Generator :: Loss: 6.172303983476013e-07]\n",
            "Epoch: 395 Label: 1, [Discriminator :: Loss: 6.357703685760498], [ Generator :: Loss: 6.175201292535348e-07]\n",
            "Epoch: 396 Label: 1, [Discriminator :: Loss: 6.498754501342773], [ Generator :: Loss: 6.229539621926961e-07]\n",
            "Epoch: 397 Label: 1, [Discriminator :: Loss: 6.255701065063477], [ Generator :: Loss: 6.293549290603551e-07]\n",
            "Epoch: 398 Label: 1, [Discriminator :: Loss: 6.3837432861328125], [ Generator :: Loss: 6.111421271270956e-07]\n",
            "Epoch: 399 Label: 1, [Discriminator :: Loss: 6.250606536865234], [ Generator :: Loss: 7.058074515953194e-07]\n"
          ]
        }
      ],
      "source": [
        "# Command Line Argument Method\n",
        "CUBE_SIDE=16\n",
        "EPOCHS = 100000\n",
        "BATCH = 64\n",
        "CHECKPOINT = 10\n",
        "LATENT_SPACE_SIZE = 256\n",
        "DATA_DIR = \"/content/3d-mnist/full_dataset_vectors.h5\"\n",
        "\n",
        "trainer = Trainer(side=CUBE_SIDE, \\\n",
        "                 latent_size=LATENT_SPACE_SIZE, \\\n",
        "                 epochs =EPOCHS,\\\n",
        "                 batch=BATCH,\\\n",
        "                 checkpoint=CHECKPOINT,\\\n",
        "                 data_dir = DATA_DIR)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vJCNVUuGmNF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3D_Gan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYxOzDX5SO4OUIA7hB9f0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}